{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea27bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1123ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''President\tDonald Trump\n",
    "Personal details\n",
    "Born\tElon Reeve Musk\n",
    "June 28, 1971 (age 53)\n",
    "Pretoria, Transvaal, South Africa\n",
    "Citizenship\t\n",
    "South Africa\n",
    "Canada (from 1989)\n",
    "United States (from 2002)\n",
    "Spouses\t\n",
    "Justine Wilson\n",
    "​\n",
    "​(m. 2000; div. 2008)​\n",
    "Talulah Riley\n",
    "​\n",
    "​(m. 2010; div. 2012)​\n",
    "​\n",
    "​(m. 2013; div. 2016)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495eba80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elon Reeve Musk'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern1=\"Born(.*)\"\n",
    "matches=re.findall(pattern1,text)\n",
    "matches[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc539cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_match(pattern,text):\n",
    "    matches=re.findall(pattern,text)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03e9693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pretoria, Transvaal, South Africa'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pattern_match('\\(age.*\\n(.*)',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e652633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_information(text):\n",
    "    age=get_pattern_match('age(\\d+)',text)\n",
    "    full_name=get_pattern_match('Born(.*)\\n',text)\n",
    "    birth_date=get_pattern_match('Born.*\\n(.*)\\(age',text)\n",
    "    birth_place=get_pattern_match('\\(age.*\\n(.*)',text)\n",
    "    \n",
    "    return {\n",
    "        'age': int(age),\n",
    "        'name':full_name.strip(),\n",
    "        'birth_date':birth_date.strip(),\n",
    "        'birth_place':birth_place.strip()\n",
    "    }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e51202",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m get_person_information(text)\n",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m, in \u001b[0;36mget_person_information\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m birth_date\u001b[38;5;241m=\u001b[39mget_pattern_match(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBorn.*\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m(.*)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(age\u001b[39m\u001b[38;5;124m'\u001b[39m,text)\n\u001b[0;32m      5\u001b[0m birth_place\u001b[38;5;241m=\u001b[39mget_pattern_match(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(age.*\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m(.*)\u001b[39m\u001b[38;5;124m'\u001b[39m,text)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(age),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m:full_name\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbirth_date\u001b[39m\u001b[38;5;124m'\u001b[39m:birth_date\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbirth_place\u001b[39m\u001b[38;5;124m'\u001b[39m:birth_place\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     12\u001b[0m }\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "get_person_information(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86dd8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dce5bb",
   "metadata": {},
   "source": [
    "1. Extract all twitter handles from following text. Twitter handle is the text that appears after https://twitter.com/ and is a single word. Also it contains only alpha numeric characters i.e. A-Z a-z , o to 9 and underscore _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8adadbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elonmusk', 'teslarati', 'dummy', 'dummy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information \n",
    "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers \n",
    "for tesla related news,\n",
    "https://twitter.com/teslarati\n",
    "https://twitter.com/dummy_tesla\n",
    "https://twitter.com/dummy_2_tesla\n",
    "'''\n",
    "pattern = 'https://twitter.com/([a-zA-Z0-9]+)' # todo: type your regex here\n",
    "\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f7876",
   "metadata": {},
   "source": [
    "2. Extract Concentration Risk Types. It will be a text that appears after \"Concentration Risk:\", In below example, your regex should extract these two strings\n",
    "\n",
    "(1) Credit Risk\n",
    "\n",
    "(2) Supply Rish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b66d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Credit Risk', 'Supply Risk']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Concentration of Risk: Credit Risk\n",
    "Financial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\n",
    "restricted cash, accounts receivable, convertible note hedges, and interest rate swaps. Our cash balances are primarily invested in money market funds\n",
    "or on deposit at high credit quality financial institutions in the U.S. These deposits are typically in excess of insured limits. As of September 30, 2021\n",
    "and December 31, 2020, no entity represented 10% or more of our total accounts receivable balance. The risk of concentration for our convertible note\n",
    "hedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\n",
    "Concentration of Risk: Supply Risk\n",
    "We are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\n",
    "products in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\n",
    "suppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\n",
    "'''\n",
    "pattern = 'Concentration of Risk: ([^\\n]*)' # todo: type your regex here\n",
    "\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d3413",
   "metadata": {},
   "source": [
    "3. Companies in europe reports their financial numbers of semi annual basis and you can have a document like this. To exatract quarterly and semin annual period you can use a regex as shown below\n",
    "\n",
    "Hint: you need to use (?:) here to match everything enclosed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef50783c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021 Q1', '2021 S1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
    "BMW's gross cost of operating vehicles in FY2021 S1 was $8 billion.\n",
    "'''\n",
    "\n",
    "pattern = 'FY(\\d{4} (?:Q[1-4]|S[1-2]))' # todo: type your regex here\n",
    "matches = re.findall(pattern, text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc101b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 653.6 kB/s eta 0:00:20\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "      --------------------------------------- 0.2/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     - -------------------------------------- 0.5/12.8 MB 2.9 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.0/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.1/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.6/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.8/12.8 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 4.9/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.0/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.9/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.9/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.2/12.8 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.4/12.8 MB 7.5 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.0/12.8 MB 7.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.0/12.8 MB 7.3 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.0/12.8 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.0/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.8/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 6.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 6.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.2/12.8 MB 6.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 6.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 5.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.5/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4d50529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3534a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of Mumbai.\n",
      "Hulk loves pani puri Chat\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc=nlp(\"Dr. Strange loves pav bhaji of Mumbai. Hulk loves pani puri Chat\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2470b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "Mumbai\n",
      ".\n",
      "Hulk\n",
      "loves\n",
      "pani\n",
      "puri\n",
      "Chat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ee7c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PRATIM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4404baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.', 'Strange loves pav bhaji of Mumbai.', 'Hulk loves pani puri Chat']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"Dr. Strange loves pav bhaji of Mumbai. Hulk loves pani puri Chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "844b035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb322d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\"\n",
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "doc = nlp(''' \"Let's go to N.Y!\"''')\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e35ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "Mumbai\n",
      ".\n",
      "Hulk\n",
      "loves\n",
      "pani\n",
      "puri\n",
      "Chat\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of Mumbai. Hulk loves pani puri Chat\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92490665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "004a4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tony gave two $ to Peter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d322d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony\n"
     ]
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "print(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54197633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afa13f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13b4d97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79a39210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\n"
     ]
    }
   ],
   "source": [
    "token1=doc[3]\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc40091f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "686646ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "gave ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "two ==> index:  2 is_alpha: True is_punct: False like_num: True is_currency: False\n",
      "$ ==> index:  3 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "to ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "Peter ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ". ==> index:  6 is_alpha: False is_punct: True like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha, \n",
    "          \"is_punct:\", token.is_punct, \n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efa69d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text=f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb73d59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dde411ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails=[]\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "        \n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "699077ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens=[token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcf864c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"}\n",
    "])\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens=[token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9304ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr. Strange loves pav bhaji of Mumbai. Hulk loves pani puri Chat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentences \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sentences)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:926\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of Mumbai. Hulk loves pani puri Chat\")\n",
    "\n",
    "for sentences in doc.sents:\n",
    "    print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89c5ee84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x20fafa19850>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44c72a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bdee26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95a0c8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x20fafa19850>)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b80485",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
    "\n",
    "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "772427aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.data.gov/\n",
      "http://www.science\n",
      "http://data.gov.uk/.\n",
      "http://www3.norc.org/gss+website/\n",
      "http://www.europeansocialsurvey.org/.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "# TODO: Write code here\n",
    "doc=nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        print(token)\n",
    "\n",
    "\n",
    "# Hint: token has an attribute that can be used to detect a url\n",
    "\n",
    "doc=nlp(text)\n",
    "\n",
    "tokens=[token.text for token in doc if token.like_url]\n",
    "tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8fc6e",
   "metadata": {},
   "source": [
    "# Figure out all transactions from this text with amount and currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a227f064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "doc = nlp(transactions)\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token.text, doc[token.i+1].text)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "168b1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5326865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "doc=nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6aeb88f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7eef84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e69b0a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x20faa0c9df0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x20faa0c9970>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x20faf8aa6c0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x20fa9021950>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x20faf949390>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x20faf8aab90>)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "556de7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain | PROPN | Captain\n",
      "america | PROPN | america\n",
      "ate | VERB | eat\n",
      "100 | NUM | 100\n",
      "$ | NUM | $\n",
      "of | ADP | of\n",
      "samosa | PROPN | samosa\n",
      ". | PUNCT | .\n",
      "Then | ADV | then\n",
      "he | PRON | he\n",
      "said | VERB | say\n",
      "I | PRON | I\n",
      "can | AUX | can\n",
      "do | VERB | do\n",
      "this | PRON | this\n",
      "all | DET | all\n",
      "day | NOUN | day\n",
      ". | PUNCT | .\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token , \"|\",token.pos_,\"|\",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109d7bf",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50ab3e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG Companies, agencies, institutions, etc.\n",
      "$45 billion MONEY Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "for token in doc.ents:\n",
    "    print(token.text,token.label_,spacy.explain(token.label_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "493cdaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc,style=\"ent\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100a09d",
   "metadata": {},
   "source": [
    "# Adding a component to a blank pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dcd507fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E944] Can't copy pipeline component 'nerd' from source 'en_core_web_sm': not found in pipeline. Available components: tok2vec, tagger, parser, senter, attribute_ruler, lemmatizer, ner\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m source_nlp\u001b[38;5;241m=\u001b[39mspacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m nlp\u001b[38;5;241m=\u001b[39mspacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnerd\u001b[39m\u001b[38;5;124m\"\u001b[39m,source\u001b[38;5;241m=\u001b[39msource_nlp)\n\u001b[0;32m      5\u001b[0m nlp\u001b[38;5;241m.\u001b[39mpipe_names\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\language.py:820\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    816\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(Warnings\u001b[38;5;241m.\u001b[39mW119\u001b[38;5;241m.\u001b[39mformat(name_in_config\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;66;03m# We're loading the component from a model. After loading the\u001b[39;00m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;66;03m# component, we know its real factory name\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     pipe_component, factory_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pipe_from_source(\n\u001b[0;32m    821\u001b[0m         factory_name, source, name\u001b[38;5;241m=\u001b[39mname\n\u001b[0;32m    822\u001b[0m     )\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    824\u001b[0m     pipe_component \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pipe(\n\u001b[0;32m    825\u001b[0m         factory_name,\n\u001b[0;32m    826\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    829\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    830\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\language.py:744\u001b[0m, in \u001b[0;36mLanguage.create_pipe_from_source\u001b[1;34m(self, source_name, source, name)\u001b[0m\n\u001b[0;32m    742\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(Warnings\u001b[38;5;241m.\u001b[39mW113\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39msource_name))\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m source\u001b[38;5;241m.\u001b[39mcomponent_names:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    745\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE944\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    746\u001b[0m             name\u001b[38;5;241m=\u001b[39msource_name,\n\u001b[0;32m    747\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    748\u001b[0m             opts\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(source\u001b[38;5;241m.\u001b[39mcomponent_names),\n\u001b[0;32m    749\u001b[0m         )\n\u001b[0;32m    750\u001b[0m     )\n\u001b[0;32m    751\u001b[0m pipe \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mget_pipe(source_name)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;66;03m# There is no actual solution here. Either the component has the right\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# name for the source pipeline or the component has the right name for\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# the current pipeline. This prioritizes the current pipeline.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"[E944] Can't copy pipeline component 'nerd' from source 'en_core_web_sm': not found in pipeline. Available components: tok2vec, tagger, parser, senter, attribute_ruler, lemmatizer, ner\""
     ]
    }
   ],
   "source": [
    "source_nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"nerd\",source=source_nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fc9b116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc ORG\n",
      "$45 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b75331",
   "metadata": {},
   "source": [
    "# Spacy Language Processing Pipelines: Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f68b9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importing necessary libraries \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  #creating an object and loading the pre-trained model for \"English\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce77d6e",
   "metadata": {},
   "source": [
    "# Excersie: 1\n",
    "\n",
    "1.Get all the proper nouns from a given text in a list and also count how many of them.\n",
    "\n",
    "2.Proper Noun means a noun that names a particular person, place, or thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f7f37b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d38a52bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_nouns_list_in_text: [Raju, Paris, London, Dubai, Rome, Mohan, Hyderabad]\n",
      "number of nouns in text: 7\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/linguistic-features\n",
    "\n",
    "#creating the nlp object\n",
    "doc = nlp(text)  \n",
    "\n",
    "#list for storing the proper nouns\n",
    "all_nouns=[]\n",
    "\n",
    "\n",
    "#checking the whether token belongs to parts of speech \"PROPN\" [Proper Noun]\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        all_nouns.append(token)\n",
    "\n",
    "\n",
    "#finally printing the results\n",
    "print(\"all_nouns_list_in_text:\",all_nouns)\n",
    "print(\"number of nouns in text:\",len(all_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0432ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_companies: [Tesla, Walmart, Amazon, Microsoft, Google, Infosys, Reliance, HDFC Bank, Hindustan Unilever, Bharti Airtel]\n",
      "number of comapnies: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = '''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "all_companies=[]\n",
    "\n",
    "for token in doc.ents:\n",
    "    if token.label_== \"ORG\":\n",
    "        all_companies.append(token)\n",
    "\n",
    "print(\"all_companies:\",all_companies)\n",
    "print(\"number of comapnies:\",len(all_companies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5982787",
   "metadata": {},
   "source": [
    "# Stemming in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3c096d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a67811fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50e89f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n"
     ]
    }
   ],
   "source": [
    "words=[\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word,\"|\",stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5073007",
   "metadata": {},
   "source": [
    "# Lemmatization in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd143300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f567c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "adjustable | adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meet\n",
      "better | well\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc=nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "\n",
    "for word in doc:\n",
    "    print(word,\"|\",word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0119dd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mando | Mando\n",
      "talked | talk\n",
      "for | for\n",
      "3 | 3\n",
      "hours | hour\n",
      "although | although\n",
      "talking | talk\n",
      "is | be\n",
      "n't | not\n",
      "his | his\n",
      "thing | thing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\"|\",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98387e6",
   "metadata": {},
   "source": [
    "# Customizing lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34f4ef2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28112bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "\n",
    "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0cf3ee45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brah"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61056151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brother'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575b6f4",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization: Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "384fed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nltk\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#for spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b220dc",
   "metadata": {},
   "source": [
    "# Exercise1:\n",
    "\n",
    "1.Convert these list of words into base form using Stemming and Lemmatization and observe the transformations\n",
    "\n",
    "2.Write a short note on the words that have different base words using stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fa245b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running | run\n",
      "painting | paint\n",
      "walking | walk\n",
      "dressing | dress\n",
      "likely | like\n",
      "children | children\n",
      "whom | whom\n",
      "good | good\n",
      "ate | ate\n",
      "fishing | fish\n"
     ]
    }
   ],
   "source": [
    "#using stemming in nltk\n",
    "lst_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']\n",
    "\n",
    "for word in lst_words:\n",
    "    print(word,\"|\",stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31eedfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running | run\n",
      "painting | painting\n",
      "walking | walking\n",
      "dressing | dress\n",
      "likely | likely\n",
      "children | child\n",
      "who | who\n",
      "good | good\n",
      "ate | eat\n",
      "fishing | fish\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#using lemmatization in spacy\n",
    "\n",
    "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\"|\",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943baa44",
   "metadata": {},
   "source": [
    "# Exercise2:\n",
    "\n",
    "convert the given text into it's base form using both stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9919fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Latha is very multi talented girl.She is good at many skills like dancing, running, singing, playing.She also likes eating Pav Bhagi. she has a \n",
    "habit of fishing and swimming too.Besides all this, she is a wonderful at cooking too.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "43106809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latha is veri multi talent girl.sh is good at mani skill like danc , run , sing , playing.sh also like eat pav bhagi . she ha a habit of fish and swim too.besid all thi , she is a wonder at cook too .\n"
     ]
    }
   ],
   "source": [
    "#using stemming in nltk\n",
    "\n",
    "\n",
    "\n",
    "#step1: Word tokenizing\n",
    "word_token=nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#step2: getting the base form for each token using stemmer\n",
    "base_words=[]\n",
    "\n",
    "for word in word_token:\n",
    "    stem_words=stemmer.stem(word)\n",
    "    base_words.append(stem_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#step3: joining all words in a list into string using 'join()'\n",
    "all_base_words=' '.join(base_words)\n",
    "print(all_base_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e578d15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latha be very multi talented girl . she be good at many skill like dancing , running , singing , play . she also like eat Pav Bhagi . she have a \n",
      " habit of fishing and swim too . besides all this , she be a wonderful at cook too . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using lemmatisation in spacy\n",
    "\n",
    "\n",
    "#step1: Creating the object for the given text\n",
    "doc=nlp(text)\n",
    "\n",
    "\n",
    "\n",
    "#step2: getting the base form for each token using spacy 'lemma_'\n",
    "base_words=[]\n",
    "\n",
    "for token in doc:\n",
    "    lemma_words=token.lemma_\n",
    "    base_words.append(lemma_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#step3: joining all words in a list into string using 'join()'\n",
    "all_lemma_words=' '.join(base_words)\n",
    "print(all_lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47dd622",
   "metadata": {},
   "source": [
    "# POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a52e467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon | PROPN | proper noun\n",
      "flew | VERB | verb\n",
      "to | ADP | adposition\n",
      "mars | NOUN | noun\n",
      "yesterday | NOUN | noun\n",
      ". | PUNCT | punctuation\n",
      "He | PRON | pronoun\n",
      "carried | VERB | verb\n",
      "biryani | ADJ | adjective\n",
      "masala | NOUN | noun\n",
      "with | ADP | adposition\n",
      "him | PRON | pronoun\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,'|',token.pos_,\"|\",spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e07607",
   "metadata": {},
   "source": [
    "# You can check https://v2.spacy.io/api/annotation for the complete list of pos categories in spacy.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Preposition_and_postposition\n",
    "\n",
    "https://en.wikipedia.org/wiki/Part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36038a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow | INTJ | interjection\n",
      "! | PUNCT | punctuation\n",
      "Dr. | PROPN | proper noun\n",
      "Strange | PROPN | proper noun\n",
      "made | VERB | verb\n",
      "265 | NUM | numeral\n",
      "million | NUM | numeral\n",
      "$ | NUM | numeral\n",
      "on | ADP | adposition\n",
      "the | DET | determiner\n",
      "very | ADV | adverb\n",
      "first | ADJ | adjective\n",
      "day | NOUN | noun\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\"|\",token.pos_,\"|\",spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac56bb",
   "metadata": {},
   "source": [
    "# Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b1b79d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow | INTJ | interjection | interjection\n",
      "! | PUNCT | punctuation | punctuation mark, sentence closer\n",
      "Dr. | PROPN | proper noun | noun, proper singular\n",
      "Strange | PROPN | proper noun | noun, proper singular\n",
      "made | VERB | verb | verb, past tense\n",
      "265 | NUM | numeral | cardinal number\n",
      "million | NUM | numeral | cardinal number\n",
      "$ | NUM | numeral | cardinal number\n",
      "on | ADP | adposition | conjunction, subordinating or preposition\n",
      "the | DET | determiner | determiner\n",
      "very | ADV | adverb | adverb\n",
      "first | ADJ | adjective | adjective (English), other noun-modifier (Chinese)\n",
      "day | NOUN | noun | noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\"|\",token.pos_,\"|\",spacy.explain(token.pos_),\"|\",spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abe817",
   "metadata": {},
   "source": [
    "# In below sentences Spacy figures out the past vs present tense for quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "21635b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quits | VBZ | verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"He quits the job\")\n",
    "\n",
    "\n",
    "print(doc[1],\"|\",doc[1].tag_,\"|\",spacy.explain(doc[1].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f96e19fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quit | VBD | verb, past tense\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He quit the job\")\n",
    "\n",
    "print(doc[1],\"|\",doc[1].tag_,\"|\",spacy.explain(doc[1].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9daa88b",
   "metadata": {},
   "source": [
    "# Removing all SPACE, PUNCT and X token from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e6e1daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·         Revenue was $51.7 billion and increased 20%\n",
    "·         Operating income was $22.2 billion and increased 24%\n",
    "·         Net income was $18.8 billion and increased 21%\n",
    "·         Diluted earnings per share was $2.48 and increased 22%\n",
    "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
    "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
    "\n",
    "doc = nlp(earnings_text)\n",
    "\n",
    "filtered_tokens=[]\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in ['SPACE','PUNCT','X']:\n",
    "        filtered_tokens.append(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9161a126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Microsoft,\n",
       " Corp.,\n",
       " today,\n",
       " announced,\n",
       " the,\n",
       " following,\n",
       " results,\n",
       " for,\n",
       " the,\n",
       " quarter,\n",
       " ended,\n",
       " December,\n",
       " 31,\n",
       " 2021,\n",
       " as,\n",
       " compared,\n",
       " to,\n",
       " the,\n",
       " corresponding,\n",
       " period]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69def102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 15,\n",
       " 92: 45,\n",
       " 100: 23,\n",
       " 90: 9,\n",
       " 85: 16,\n",
       " 93: 16,\n",
       " 97: 27,\n",
       " 98: 1,\n",
       " 84: 20,\n",
       " 103: 10,\n",
       " 87: 6,\n",
       " 99: 5,\n",
       " 89: 12,\n",
       " 86: 3,\n",
       " 94: 3,\n",
       " 95: 2}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "161e4368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab[96].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b21907e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 15\n",
      "NOUN | 45\n",
      "VERB | 23\n",
      "DET | 9\n",
      "ADP | 16\n",
      "NUM | 16\n",
      "PUNCT | 27\n",
      "SCONJ | 1\n",
      "ADJ | 20\n",
      "SPACE | 10\n",
      "AUX | 6\n",
      "SYM | 5\n",
      "CCONJ | 12\n",
      "ADV | 3\n",
      "PART | 3\n",
      "PRON | 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text,\"|\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d7ba7",
   "metadata": {},
   "source": [
    "# Exercise for Spacy POS tutorial,\n",
    "\n",
    "You are parsing a news story from cnbc.com. News story is stores in news_story.txt which is available in this same folder on github. You need to,\n",
    "\n",
    "1.Extract all NOUN tokens from this story. You will have to read the file in python first to collect all the text and then extract NOUNs in a python list\n",
    "\n",
    "2.Extract all numbers (NUM POS type) in a python list\n",
    "\n",
    "3.Print a count of all POS tags in this story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1dce73ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inflation rose again in April, continuing a climb that has pushed consumers to the brink and is threatening the economic expansion, the Bureau of Labor Statistics reported Wednesday.\\n\\nThe consumer price index, a broad-based measure of prices for goods and services, increased 8.3% from a year ago, higher than the Dow Jones estimate for an 8.1% gain. That represented a slight ease from Marchâ€™s peak but was still close to the highest level since the summer of 1982.\\n\\nRemoving volatile food and ene'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"news_story.txt\") as f:\n",
    "    text=f.read()\n",
    "\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36e9e1",
   "metadata": {},
   "source": [
    "# Read a new story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "03653e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936292d2",
   "metadata": {},
   "source": [
    "# Extract NOUN and NUM token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "53fedb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=nlp(text)\n",
    "\n",
    "numbers=[]\n",
    "nouns=[]\n",
    "\n",
    "for token in docs:\n",
    "    if token.pos_ == \"NUM\":\n",
    "        numbers.append(token)\n",
    "    elif token.pos_ == \"NOUN\":\n",
    "        nouns.append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c993d98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.3, 8.1, 1982, 6.2, 6, 0.3, 0.2, 0.6, 0.4, 0.1]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e52341a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Inflation,\n",
       " climb,\n",
       " consumers,\n",
       " brink,\n",
       " expansion,\n",
       " consumer,\n",
       " price,\n",
       " index,\n",
       " measure,\n",
       " prices]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae72efe",
   "metadata": {},
   "source": [
    "# Print a count of all POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f4b51cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 15,\n",
       " 92: 45,\n",
       " 100: 23,\n",
       " 90: 9,\n",
       " 85: 16,\n",
       " 93: 16,\n",
       " 97: 27,\n",
       " 98: 1,\n",
       " 84: 20,\n",
       " 103: 10,\n",
       " 87: 6,\n",
       " 99: 5,\n",
       " 89: 12,\n",
       " 86: 3,\n",
       " 94: 3,\n",
       " 95: 2}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=doc.count_by(spacy.attrs.POS)\n",
    "count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d27683c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 15\n",
      "NOUN | 45\n",
      "VERB | 23\n",
      "DET | 9\n",
      "ADP | 16\n",
      "NUM | 16\n",
      "PUNCT | 27\n",
      "SCONJ | 1\n",
      "ADJ | 20\n",
      "SPACE | 10\n",
      "AUX | 6\n",
      "SYM | 5\n",
      "CCONJ | 12\n",
      "ADV | 3\n",
      "PART | 3\n",
      "PRON | 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text,\"|\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458ac61",
   "metadata": {},
   "source": [
    "# NLP Tutorial: Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "422cf43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "07c23bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "85f6bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc | ORG | Companies, agencies, institutions, etc.\n",
      "$45 billion | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "for token in doc.ents:\n",
    "    print(token.text,\"|\",token.label_,\"|\",spacy.explain(token.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7a7bba61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6197f",
   "metadata": {},
   "source": [
    "# List down all the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19db305c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0fe77d",
   "metadata": {},
   "source": [
    "List of entities are also documented on this page: https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "81d1c693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Bloomberg | PERSON | People, including fictional\n",
      "Bloomberg | GPE | Countries, cities, states\n",
      "1982 | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Michael Bloomberg founded Bloomberg in 1982\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\"|\",ent.label_,\"|\",spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c005e2",
   "metadata": {},
   "source": [
    "Above it made a mistake in identifying Bloomberg the company. Let's try hugging face for this now.\n",
    "\n",
    "https://huggingface.co/dslim/bert-base-NER?text=Michael+Bloomberg+founded+Bloomberg+in+1982\n",
    "\n",
    "Here also go through 3 sample examples for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4a25d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  0 | 9\n",
      "Twitter Inc  |  PERSON  |  30 | 41\n",
      "$45 billion  |  MONEY  |  46 | 57\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", ent.start_char, \"|\", ent.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63e535c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is going to"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=doc[2:5]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4abb4e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8dbe5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc,0,1,label='ORG')\n",
    "s2 = Span(doc,5,6,label=\"ORG\")\n",
    "\n",
    "doc.set_ents([s1,s2],default=\"unmodified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1cc76ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc | ORG\n",
      "acquire | ORG\n",
      "Twitter Inc | PERSON\n",
      "$45 billion | MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,\"|\",ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1b2f3",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER): Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc57659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  #creating an object and loading the pre-trained model for \"English\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96aca07",
   "metadata": {},
   "source": [
    "# Excersie: 1\n",
    "\n",
    "Extract all the Geographical (cities, Countries, states) names from a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "84288843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ec58a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geographicnames: [Kiran, India, Delhi, Gujarat, Tamilnadu, Andhrapradesh, Assam, Bihar]\n",
      "count: 8\n"
     ]
    }
   ],
   "source": [
    "geographic_names=[]\n",
    "\n",
    "for token in doc.ents:\n",
    "    if token.label_ == 'GPE':\n",
    "        geographic_names.append(token)\n",
    "    \n",
    "print(\"geographicnames:\",geographic_names)\n",
    "print(\"count:\",len(geographic_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f736be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allbirthdates: [24 April 1973, 5 November 1988, 7 July 1981, 19 December 1974]\n",
      "Count: 4\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kholi was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
    "and finally Ricky ponting was born on 19 December 1974.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "all_date_births=[]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"DATE\":\n",
    "        all_date_births.append(ent)\n",
    "    \n",
    "print(\"allbirthdates:\",all_date_births)\n",
    "print(\"Count:\",len(all_date_births))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82507e4",
   "metadata": {},
   "source": [
    "# NLP Tutorial: Text Representation - Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1617de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "23cc4d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1e56184a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "65af0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spam']=df['Category'].apply(lambda x: 1 if x == \"spam\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dd0fde7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message  spam\n",
       "0      ham  Go until jurong point, crazy.. Available only ...     0\n",
       "1      ham                      Ok lar... Joking wif u oni...     0\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
       "3      ham  U dun say so early hor... U c already then say...     0\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...     0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1d0f3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df.Message,df.spam,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6584e338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b48d613d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "568ab1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "66946c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5311    K.k:)i'm going to tirunelvali this week to see...\n",
       "2264    Not heard from U4 a while. Call 4 rude chat pr...\n",
       "4209    Or i go home first lar ü wait 4 me lor.. I put...\n",
       "5387    I will be gentle baby! Soon you will be taking...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "09f28771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "61911441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5311    0\n",
       "2264    1\n",
       "4209    0\n",
       "5387    0\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "534b5b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b078b5",
   "metadata": {},
   "source": [
    "# Create bag of words representation using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "83ea1693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4457x7750 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 59161 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = CountVectorizer()\n",
    "\n",
    "x_train_cv=v.fit_transform(X_train.values)\n",
    "x_train_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d42acfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cv.toarray()[:2][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28bdae79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7750)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "56ed8f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'childrens'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.get_feature_names_out()[1771]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bedb2279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__sklearn_clone__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_build_request_for_signature',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_check_stop_words_consistency',\n",
       " '_check_vocabulary',\n",
       " '_count_vocab',\n",
       " '_get_default_requests',\n",
       " '_get_metadata_request',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_limit_features',\n",
       " '_more_tags',\n",
       " '_parameter_constraints',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_sort_features',\n",
       " '_stop_words_id',\n",
       " '_validate_data',\n",
       " '_validate_ngram_range',\n",
       " '_validate_params',\n",
       " '_validate_vocabulary',\n",
       " '_warn_for_unused_params',\n",
       " '_white_spaces',\n",
       " '_word_ngrams',\n",
       " 'analyzer',\n",
       " 'binary',\n",
       " 'build_analyzer',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'decode',\n",
       " 'decode_error',\n",
       " 'dtype',\n",
       " 'encoding',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'fixed_vocabulary_',\n",
       " 'get_feature_names_out',\n",
       " 'get_metadata_routing',\n",
       " 'get_params',\n",
       " 'get_stop_words',\n",
       " 'input',\n",
       " 'inverse_transform',\n",
       " 'lowercase',\n",
       " 'max_df',\n",
       " 'max_features',\n",
       " 'min_df',\n",
       " 'ngram_range',\n",
       " 'preprocessor',\n",
       " 'set_fit_request',\n",
       " 'set_params',\n",
       " 'set_transform_request',\n",
       " 'stop_words',\n",
       " 'stop_words_',\n",
       " 'strip_accents',\n",
       " 'token_pattern',\n",
       " 'tokenizer',\n",
       " 'transform',\n",
       " 'vocabulary',\n",
       " 'vocabulary_']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "121fdbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'going': 3161,\n",
       " 'to': 6941,\n",
       " 'tirunelvali': 6918,\n",
       " 'this': 6859,\n",
       " 'week': 7446,\n",
       " 'see': 5995,\n",
       " 'my': 4685,\n",
       " 'uncle': 7151,\n",
       " 'already': 917,\n",
       " 'spend': 6387,\n",
       " 'the': 6817,\n",
       " 'amount': 941,\n",
       " 'by': 1570,\n",
       " 'taking': 6698,\n",
       " 'dress': 2441,\n",
       " 'so': 6291,\n",
       " 'only': 4959,\n",
       " 'want': 7388,\n",
       " 'money': 4586,\n",
       " 'will': 7527,\n",
       " 'give': 3132,\n",
       " 'it': 3761,\n",
       " 'on': 4949,\n",
       " 'feb': 2782,\n",
       " 'not': 4847,\n",
       " 'heard': 3374,\n",
       " 'from': 3004,\n",
       " 'u4': 7127,\n",
       " 'while': 7498,\n",
       " 'call': 1586,\n",
       " 'rude': 5855,\n",
       " 'chat': 1727,\n",
       " 'private': 5433,\n",
       " 'line': 4135,\n",
       " '01223585334': 6,\n",
       " 'cum': 2096,\n",
       " 'wan': 7383,\n",
       " '2c': 384,\n",
       " 'pics': 5218,\n",
       " 'of': 4908,\n",
       " 'me': 4417,\n",
       " 'gettin': 3110,\n",
       " 'shagged': 6066,\n",
       " 'then': 6829,\n",
       " 'text': 6789,\n",
       " 'pix': 5237,\n",
       " '8552': 675,\n",
       " '2end': 388,\n",
       " 'send': 6023,\n",
       " 'stop': 6502,\n",
       " 'sam': 5902,\n",
       " 'xxx': 7662,\n",
       " 'or': 4986,\n",
       " 'go': 3150,\n",
       " 'home': 3468,\n",
       " 'first': 2850,\n",
       " 'lar': 4026,\n",
       " 'wait': 7365,\n",
       " 'lor': 4218,\n",
       " 'put': 5530,\n",
       " 'down': 2425,\n",
       " 'stuff': 6551,\n",
       " 'be': 1259,\n",
       " 'gentle': 3094,\n",
       " 'baby': 1177,\n",
       " 'soon': 6330,\n",
       " 'you': 7712,\n",
       " 'all': 906,\n",
       " 'lt': 4263,\n",
       " 'gt': 3251,\n",
       " 'inches': 3642,\n",
       " 'deep': 2201,\n",
       " 'inside': 3691,\n",
       " 'your': 7718,\n",
       " 'tight': 6899,\n",
       " 'pussy': 5529,\n",
       " 'hi': 3419,\n",
       " 'chachi': 1696,\n",
       " 'tried': 7049,\n",
       " 'calling': 1599,\n",
       " 'now': 4860,\n",
       " 'unable': 7147,\n",
       " 'reach': 5611,\n",
       " 'pl': 5240,\n",
       " 'missed': 4523,\n",
       " 'cal': 1581,\n",
       " 'once': 4952,\n",
       " 'tiz': 6926,\n",
       " 'msg': 4635,\n",
       " 'kanagu': 3894,\n",
       " 'no': 4812,\n",
       " 'we': 7425,\n",
       " 'sell': 6015,\n",
       " 'll': 4173,\n",
       " 'have': 3352,\n",
       " 'tons': 6977,\n",
       " 'if': 3598,\n",
       " 'coins': 1866,\n",
       " 'our': 5019,\n",
       " 'someone': 6309,\n",
       " 'thru': 6885,\n",
       " 'paypal': 5140,\n",
       " 'voila': 7332,\n",
       " 'back': 1181,\n",
       " 'in': 3638,\n",
       " 'life': 4110,\n",
       " 'pockets': 5293,\n",
       " 'gudnite': 3256,\n",
       " 'tc': 6728,\n",
       " 'practice': 5369,\n",
       " 'ok': 4930,\n",
       " 'is': 3749,\n",
       " 'any': 985,\n",
       " 'problem': 5443,\n",
       " 'frm': 2994,\n",
       " 'him': 3428,\n",
       " 'wats': 7417,\n",
       " 'matter': 4401,\n",
       " 'at': 1106,\n",
       " 'funeral': 3032,\n",
       " 'with': 7555,\n",
       " 'audrey': 1128,\n",
       " 'and': 952,\n",
       " 'dad': 2132,\n",
       " 'too': 6978,\n",
       " 'much': 4652,\n",
       " 'close': 1833,\n",
       " 'heart': 3377,\n",
       " 'away': 1160,\n",
       " 'shattered': 6084,\n",
       " 'plz': 5276,\n",
       " 'stay': 6470,\n",
       " 've': 7275,\n",
       " 'reached': 5613,\n",
       " 'finally': 2830,\n",
       " 'am': 928,\n",
       " 'great': 3226,\n",
       " 'princess': 5426,\n",
       " 'what': 7484,\n",
       " 'are': 1041,\n",
       " 'thinking': 6853,\n",
       " 'about': 752,\n",
       " 'had': 3281,\n",
       " 'issue': 3758,\n",
       " 'weigh': 7452,\n",
       " 'but': 1556,\n",
       " 'thanks': 6807,\n",
       " 'can': 1610,\n",
       " 'breathe': 1476,\n",
       " 'easier': 2502,\n",
       " 'make': 4340,\n",
       " 'sure': 6631,\n",
       " 'dont': 2407,\n",
       " 'regret': 5685,\n",
       " 'dear': 2180,\n",
       " 'voucher': 7340,\n",
       " 'holder': 3460,\n",
       " 'claim': 1805,\n",
       " 'weeks': 7451,\n",
       " 'offer': 4913,\n",
       " 'pc': 5141,\n",
       " 'please': 5264,\n",
       " 'http': 3532,\n",
       " 'www': 7647,\n",
       " 'wtlp': 7641,\n",
       " 'co': 1851,\n",
       " 'uk': 7137,\n",
       " 'ts': 7073,\n",
       " 'cs': 2077,\n",
       " 'apply': 1018,\n",
       " 'meet': 4435,\n",
       " 'sexy': 6058,\n",
       " 'today': 6947,\n",
       " 'find': 2832,\n",
       " 'date': 2160,\n",
       " 'even': 2644,\n",
       " 'flirt': 2878,\n",
       " 'its': 3768,\n",
       " 'up': 7199,\n",
       " 'join': 3840,\n",
       " 'just': 3877,\n",
       " '10p': 267,\n",
       " 'reply': 5729,\n",
       " 'name': 4705,\n",
       " 'age': 856,\n",
       " 'eg': 2534,\n",
       " '25': 367,\n",
       " '18': 319,\n",
       " 'recd': 5638,\n",
       " 'thirtyeight': 6857,\n",
       " 'pence': 5153,\n",
       " 'best': 1315,\n",
       " 'ur': 7217,\n",
       " 'exam': 2677,\n",
       " 'later': 4038,\n",
       " 'oh': 4925,\n",
       " 'sorry': 6338,\n",
       " 'over': 5039,\n",
       " 'for': 2922,\n",
       " 'information': 3673,\n",
       " 'ikea': 3606,\n",
       " 'spelled': 6385,\n",
       " 'caps': 1626,\n",
       " 'that': 6813,\n",
       " 'yelling': 7689,\n",
       " 'when': 7490,\n",
       " 'thought': 6870,\n",
       " 'left': 4076,\n",
       " 'were': 7471,\n",
       " 'sitting': 6199,\n",
       " 'bed': 1274,\n",
       " 'among': 938,\n",
       " 'mess': 4466,\n",
       " 'came': 1605,\n",
       " 'said': 5891,\n",
       " 'after': 846,\n",
       " 'got': 3191,\n",
       " 'class': 1813,\n",
       " 'don': 2403,\n",
       " 'try': 7070,\n",
       " 'bullshit': 1539,\n",
       " 'makes': 4341,\n",
       " 'listen': 4155,\n",
       " 'less': 4090,\n",
       " 'da': 2128,\n",
       " 'thangam': 6805,\n",
       " 'very': 7287,\n",
       " 'held': 3388,\n",
       " 'prasad': 5378,\n",
       " 'college': 1879,\n",
       " 'able': 751,\n",
       " 'atten': 1117,\n",
       " 'fact': 2730,\n",
       " 're': 5610,\n",
       " 'cleaning': 1820,\n",
       " 'shows': 6146,\n",
       " 'know': 3965,\n",
       " 'why': 7511,\n",
       " 'upset': 7213,\n",
       " 'priority': 5430,\n",
       " 'constantly': 1960,\n",
       " 'do': 2375,\n",
       " 'need': 4748,\n",
       " 'temales': 6762,\n",
       " 'was': 7401,\n",
       " 'wonderful': 7586,\n",
       " 'thank': 6806,\n",
       " 'little': 4164,\n",
       " 'meds': 4433,\n",
       " 'say': 5940,\n",
       " 'take': 6693,\n",
       " 'every': 2651,\n",
       " 'hours': 3512,\n",
       " 'been': 1280,\n",
       " 'pain': 5068,\n",
       " 'took': 6979,\n",
       " 'another': 971,\n",
       " 'hope': 3485,\n",
       " 'die': 2302,\n",
       " 'received': 5642,\n",
       " 'understood': 7163,\n",
       " 'acted': 794,\n",
       " 'upon': 7210,\n",
       " 'yo': 7707,\n",
       " 'come': 1890,\n",
       " 'carlos': 1644,\n",
       " 'here': 3408,\n",
       " 'chinatown': 1777,\n",
       " 'porridge': 5328,\n",
       " 'claypot': 1817,\n",
       " 'rice': 5787,\n",
       " 'yam': 7675,\n",
       " 'cake': 1579,\n",
       " 'fishhead': 2852,\n",
       " 'beehoon': 1279,\n",
       " 'either': 2543,\n",
       " 'eat': 2507,\n",
       " 'cheap': 1732,\n",
       " 'den': 2233,\n",
       " 'cafe': 1577,\n",
       " 'tok': 6954,\n",
       " 'nydc': 4886,\n",
       " 'somethin': 6312,\n",
       " 'didn': 2299,\n",
       " 'lunch': 4273,\n",
       " 'fighting': 2812,\n",
       " 'world': 7602,\n",
       " 'easy': 2506,\n",
       " 'win': 7530,\n",
       " 'lose': 4220,\n",
       " 'bt': 1520,\n",
       " 'fightng': 2813,\n",
       " 'some1': 6306,\n",
       " 'who': 7503,\n",
       " 'dificult': 2314,\n",
       " 'still': 6490,\n",
       " 'contact': 1961,\n",
       " 'video': 7295,\n",
       " 'handset': 3306,\n",
       " '750': 616,\n",
       " 'anytime': 996,\n",
       " 'networks': 4771,\n",
       " 'mins': 4508,\n",
       " 'unlimited': 7187,\n",
       " 'camcorder': 1604,\n",
       " '08000930705': 50,\n",
       " 'bold': 1406,\n",
       " 'bb': 1242,\n",
       " 'torch': 6994,\n",
       " 'hostel': 3503,\n",
       " 'sleep': 6224,\n",
       " 'before': 1285,\n",
       " 'hrishi': 3529,\n",
       " 'complimentary': 1917,\n",
       " 'star': 6453,\n",
       " 'ibiza': 3573,\n",
       " 'holiday': 3463,\n",
       " '10': 255,\n",
       " '000': 1,\n",
       " 'cash': 1655,\n",
       " 'needs': 4752,\n",
       " 'urgent': 7221,\n",
       " 'collection': 1877,\n",
       " '09066364349': 228,\n",
       " 'landline': 4013,\n",
       " 'out': 5022,\n",
       " 'box434sk38wp150ppm18': 1446,\n",
       " 'gr8': 3203,\n",
       " 'message': 4467,\n",
       " 'leaving': 4072,\n",
       " 'congrats': 1944,\n",
       " 'school': 5952,\n",
       " 'wat': 7408,\n",
       " 'plans': 5252,\n",
       " 'new': 4776,\n",
       " 'phone': 5202,\n",
       " 'network': 4769,\n",
       " 'half': 3293,\n",
       " 'price': 5421,\n",
       " 'rental': 5718,\n",
       " 'free': 2967,\n",
       " 'months': 4598,\n",
       " 'delivery': 2228,\n",
       " 'mobile': 4558,\n",
       " '3g': 460,\n",
       " 'videophones': 7297,\n",
       " 'yours': 7722,\n",
       " '09063458130': 207,\n",
       " 'videochat': 7296,\n",
       " 'wid': 7515,\n",
       " 'mates': 4394,\n",
       " 'play': 5255,\n",
       " 'java': 3801,\n",
       " 'games': 3056,\n",
       " 'dload': 2372,\n",
       " 'polyph': 5311,\n",
       " 'music': 4674,\n",
       " 'noline': 4825,\n",
       " 'rentl': 5720,\n",
       " 'tot': 7000,\n",
       " 'outside': 5033,\n",
       " 'cos': 2001,\n",
       " 'darren': 2155,\n",
       " 'shopping': 6122,\n",
       " 'course': 2022,\n",
       " 'nice': 4787,\n",
       " 'jus': 3876,\n",
       " 'went': 7468,\n",
       " 'sim': 6175,\n",
       " 'lim': 4130,\n",
       " 'look': 4207,\n",
       " 'mp3': 4628,\n",
       " 'player': 5257,\n",
       " 'byatch': 1571,\n",
       " 'whassup': 7483,\n",
       " 'alright': 918,\n",
       " 'hooked': 3482,\n",
       " 'where': 7493,\n",
       " 'guys': 3270,\n",
       " 'orchard': 4992,\n",
       " '1st': 334,\n",
       " 'wk': 7564,\n",
       " 'tones': 6971,\n",
       " 'str8': 6518,\n",
       " 'each': 2492,\n",
       " 'txt': 7109,\n",
       " 'nokia': 4821,\n",
       " '8007': 636,\n",
       " 'classic': 1815,\n",
       " 'hit': 3437,\n",
       " 'polys': 5313,\n",
       " '150p': 304,\n",
       " 'poly': 5307,\n",
       " '200p': 353,\n",
       " '16': 315,\n",
       " 'effect': 2531,\n",
       " 'irritation': 3747,\n",
       " 'ignore': 3603,\n",
       " 'he': 3363,\n",
       " 'neva': 4772,\n",
       " 'grumble': 3247,\n",
       " 'sad': 5884,\n",
       " 'hee': 3384,\n",
       " 'buy': 1560,\n",
       " 'tmr': 6935,\n",
       " 'aft': 845,\n",
       " 'meetin': 4436,\n",
       " 'hear': 3373,\n",
       " 'fr': 2955,\n",
       " 'them': 6825,\n",
       " 'lei': 4081,\n",
       " 'lot': 4226,\n",
       " 'work': 7597,\n",
       " 'ar': 1035,\n",
       " 'well': 7462,\n",
       " 'welp': 7464,\n",
       " 'sort': 6339,\n",
       " 'semiobscure': 6021,\n",
       " 'internet': 3713,\n",
       " 'thing': 6848,\n",
       " 'good': 3173,\n",
       " 'job': 3833,\n",
       " 'like': 4122,\n",
       " 'entrepreneurs': 2598,\n",
       " 'workin': 7599,\n",
       " 'overtime': 5044,\n",
       " 'nigpun': 4798,\n",
       " 'neft': 4755,\n",
       " 'transaction': 7026,\n",
       " 'reference': 5667,\n",
       " 'number': 4873,\n",
       " 'rs': 5847,\n",
       " 'decimal': 2193,\n",
       " 'has': 3337,\n",
       " 'credited': 2054,\n",
       " 'beneficiary': 1310,\n",
       " 'account': 781,\n",
       " 'time': 6906,\n",
       " 'natalja': 4723,\n",
       " 'inviting': 3728,\n",
       " 'her': 3407,\n",
       " 'friend': 2988,\n",
       " 'yes': 7695,\n",
       " '440': 493,\n",
       " 'sms': 6271,\n",
       " 'ac': 764,\n",
       " 'nat27081980': 4720,\n",
       " 'frnd': 2996,\n",
       " '62468': 583,\n",
       " 'current': 2104,\n",
       " 'leading': 4060,\n",
       " 'bid': 1332,\n",
       " '151': 311,\n",
       " 'pause': 5130,\n",
       " 'auction': 1125,\n",
       " 'customer': 2112,\n",
       " 'care': 1632,\n",
       " '08718726270': 139,\n",
       " 'taste': 6714,\n",
       " 'fish': 2851,\n",
       " 'curry': 2106,\n",
       " 'afternoon': 848,\n",
       " 'glorious': 3141,\n",
       " 'anniversary': 964,\n",
       " 'day': 2167,\n",
       " 'sweet': 6656,\n",
       " 'finds': 2834,\n",
       " 'happy': 3326,\n",
       " 'content': 1965,\n",
       " 'prey': 5420,\n",
       " 'think': 6850,\n",
       " 'teasing': 6743,\n",
       " 'kiss': 3952,\n",
       " 'across': 793,\n",
       " 'sea': 5978,\n",
       " 'coaxing': 1854,\n",
       " 'images': 3613,\n",
       " 'fond': 2906,\n",
       " 'souveniers': 6355,\n",
       " 'cougar': 2008,\n",
       " 'pen': 5152,\n",
       " 'yup': 7737,\n",
       " 'right': 5792,\n",
       " 'strike': 6529,\n",
       " 'red': 5662,\n",
       " 'one': 4954,\n",
       " 'bird': 1348,\n",
       " 'antelope': 980,\n",
       " 'begin': 1290,\n",
       " 'toplay': 6989,\n",
       " 'fieldof': 2807,\n",
       " 'selfindependence': 6012,\n",
       " 'believe': 1297,\n",
       " 'flower': 2884,\n",
       " 'contention': 1967,\n",
       " 'grow': 3243,\n",
       " 'random': 5586,\n",
       " 'sick': 6159,\n",
       " 'eh': 2538,\n",
       " 'ger': 3101,\n",
       " 'toking': 6956,\n",
       " 'abt': 760,\n",
       " 'syd': 6671,\n",
       " 'leh': 4080,\n",
       " 'haha': 3284,\n",
       " 'waiting': 7368,\n",
       " 'tv': 7097,\n",
       " 'show': 6139,\n",
       " 'start': 6457,\n",
       " 'busy': 1555,\n",
       " 'doing': 2394,\n",
       " 'report': 5732,\n",
       " 'theyre': 6844,\n",
       " 'lots': 4228,\n",
       " 'places': 5244,\n",
       " 'hospitals': 3501,\n",
       " 'medical': 4431,\n",
       " 'safe': 5887,\n",
       " 'loud': 4234,\n",
       " 'scream': 5966,\n",
       " 'minutes': 4511,\n",
       " 'cause': 1668,\n",
       " 'gyno': 3276,\n",
       " 'shoving': 6138,\n",
       " 'things': 6849,\n",
       " 'belong': 1303,\n",
       " 'moment': 4580,\n",
       " 'own': 5051,\n",
       " 'value': 7261,\n",
       " 'morning': 4606,\n",
       " 'brings': 1492,\n",
       " 'faith': 2743,\n",
       " 'evening': 2645,\n",
       " 'luv': 4278,\n",
       " 'night': 4794,\n",
       " 'rest': 5758,\n",
       " 'wish': 7547,\n",
       " 'important': 3627,\n",
       " 'orange': 4989,\n",
       " 'user': 7236,\n",
       " 'lucky': 4269,\n",
       " '2find': 390,\n",
       " 'log': 4191,\n",
       " 'onto': 4962,\n",
       " 'urawinner': 7218,\n",
       " 'com': 1884,\n",
       " 'there': 6835,\n",
       " 'fantastic': 2760,\n",
       " 'surprise': 6637,\n",
       " 'awaiting': 1156,\n",
       " 'bored': 1422,\n",
       " 'aah': 739,\n",
       " 'cuddle': 2090,\n",
       " 'would': 7617,\n",
       " 'lush': 4276,\n",
       " 'tea': 6732,\n",
       " 'soup': 6350,\n",
       " 'kind': 3943,\n",
       " 'fumbling': 3026,\n",
       " 'wake': 7369,\n",
       " 'long': 4203,\n",
       " 'ago': 864,\n",
       " 'dunno': 2483,\n",
       " 'other': 5012,\n",
       " 'house': 3513,\n",
       " 'how': 3518,\n",
       " 'pictures': 5221,\n",
       " 'facebook': 2728,\n",
       " 'trying': 7072,\n",
       " 'last': 4032,\n",
       " 'weekends': 7449,\n",
       " 'draw': 2435,\n",
       " 'won': 7583,\n",
       " '900': 712,\n",
       " 'prize': 5436,\n",
       " 'guaranteed': 3252,\n",
       " '09061701851': 193,\n",
       " 'code': 1861,\n",
       " 'k61': 3884,\n",
       " 'valid': 7258,\n",
       " '12hours': 286,\n",
       " 'whore': 7507,\n",
       " 'unbelievable': 7148,\n",
       " 'fyi': 3042,\n",
       " 'usf': 7237,\n",
       " 'swing': 6664,\n",
       " 'room': 5830,\n",
       " 'whenever': 7491,\n",
       " 'late': 4035,\n",
       " 'especially': 2628,\n",
       " 'wednesday': 7443,\n",
       " 'probably': 5442,\n",
       " 'failed': 2737,\n",
       " 'test': 6783,\n",
       " 'friday': 2984,\n",
       " 'juz': 3882,\n",
       " 'remembered': 5702,\n",
       " 'gotta': 3197,\n",
       " 'bathe': 1233,\n",
       " 'dog': 2388,\n",
       " 'called': 1593,\n",
       " 'mom': 4579,\n",
       " 'instead': 3699,\n",
       " 'fun': 3027,\n",
       " '86688': 679,\n",
       " 'let': 4094,\n",
       " 'rcvd': 5607,\n",
       " 'hg': 3418,\n",
       " 'suite342': 6598,\n",
       " '2lands': 397,\n",
       " 'row': 5841,\n",
       " 'w1j6hl': 7351,\n",
       " 'ldn': 4055,\n",
       " 'years': 7686,\n",
       " 'did': 2297,\n",
       " 'fine': 2835,\n",
       " 'smile': 6259,\n",
       " 'roommate': 5831,\n",
       " 'also': 920,\n",
       " 'wants': 7392,\n",
       " 'dubsack': 2472,\n",
       " 'may': 4409,\n",
       " 'some': 6305,\n",
       " 'plan': 5246,\n",
       " 'bringing': 1491,\n",
       " 'extra': 2717,\n",
       " 'tell': 6755,\n",
       " 'they': 6843,\n",
       " 'package': 5060,\n",
       " 'programs': 5461,\n",
       " 'sister': 6194,\n",
       " 'placed': 5242,\n",
       " 'birla': 1350,\n",
       " 'soft': 6296,\n",
       " 'search': 5979,\n",
       " 'happiness': 3325,\n",
       " 'main': 4336,\n",
       " 'sources': 6352,\n",
       " 'unhappiness': 7170,\n",
       " 'accept': 769,\n",
       " 'way': 7421,\n",
       " 'comes': 1892,\n",
       " 'live': 4165,\n",
       " 'dun': 2481,\n",
       " 'pick': 5214,\n",
       " 'gf': 3113,\n",
       " 'drive': 2449,\n",
       " 'sch': 5950,\n",
       " 'pls': 5270,\n",
       " 'address': 813,\n",
       " 'sir': 6192,\n",
       " 'since': 6181,\n",
       " 'screamed': 5967,\n",
       " 'aight': 875,\n",
       " 'lemme': 4083,\n",
       " 'miserable': 4517,\n",
       " 'eatin': 2509,\n",
       " 'watching': 7412,\n",
       " 'europe': 2640,\n",
       " 'xmas': 7656,\n",
       " 'story': 6516,\n",
       " 'peace': 5144,\n",
       " 'love': 4239,\n",
       " 'miracle': 4513,\n",
       " 'jesus': 3821,\n",
       " 'hav': 3348,\n",
       " 'blessed': 1375,\n",
       " 'month': 4595,\n",
       " 'ahead': 868,\n",
       " 'amp': 942,\n",
       " 'merry': 4464,\n",
       " 'means': 4424,\n",
       " 'fat': 2768,\n",
       " 'head': 3364,\n",
       " 'uncles': 7152,\n",
       " 'atlanta': 1110,\n",
       " 'semester': 6019,\n",
       " 'yar': 7677,\n",
       " 'knew': 3962,\n",
       " 'dis': 2337,\n",
       " 'happen': 3316,\n",
       " 'should': 6133,\n",
       " 'straight': 6519,\n",
       " 'as': 1077,\n",
       " 'usual': 7242,\n",
       " 'iam': 3569,\n",
       " 'per': 5158,\n",
       " 'request': 5735,\n",
       " 'melle': 4446,\n",
       " 'oru': 5007,\n",
       " 'minnaminunginte': 4506,\n",
       " 'nurungu': 4876,\n",
       " 'vettam': 7288,\n",
       " 'set': 6047,\n",
       " 'callertune': 1596,\n",
       " 'callers': 1595,\n",
       " 'press': 5409,\n",
       " 'copy': 1992,\n",
       " 'friends': 2989,\n",
       " 'thats': 6816,\n",
       " 'cool': 1987,\n",
       " 'cover': 2026,\n",
       " 'face': 2727,\n",
       " 'hot': 3504,\n",
       " 'sticky': 6487,\n",
       " 'watch': 7409,\n",
       " 'pm': 5277,\n",
       " 'gone': 3169,\n",
       " 'into': 3718,\n",
       " 'get': 3103,\n",
       " 'info': 3671,\n",
       " 'history': 3436,\n",
       " 'comfort': 1893,\n",
       " 'luxury': 4282,\n",
       " 'sold': 6302,\n",
       " 'same': 5905,\n",
       " 'india': 3661,\n",
       " 'onion': 4957,\n",
       " 'petrol': 5188,\n",
       " 'beer': 1282,\n",
       " 'shesil': 6095,\n",
       " 'height': 3387,\n",
       " 'confidence': 1937,\n",
       " 'aeronautics': 832,\n",
       " 'professors': 5456,\n",
       " 'wer': 7470,\n",
       " 'calld': 1592,\n",
       " 'askd': 1086,\n",
       " 'sit': 6196,\n",
       " 'an': 950,\n",
       " 'aeroplane': 833,\n",
       " 'aftr': 851,\n",
       " 'sat': 5923,\n",
       " 'told': 6957,\n",
       " 'dat': 2159,\n",
       " 'plane': 5247,\n",
       " 'ws': 7635,\n",
       " 'made': 4316,\n",
       " 'their': 6824,\n",
       " 'students': 6546,\n",
       " 'dey': 2281,\n",
       " 'hurried': 3553,\n",
       " 'didnt': 2300,\n",
       " 'move': 4621,\n",
       " 'wont': 7589,\n",
       " 'datz': 2164,\n",
       " 'willing': 7528,\n",
       " 'aptitude': 1033,\n",
       " 'lol': 4199,\n",
       " 'real': 5621,\n",
       " 'she': 6088,\n",
       " 'cancer': 1617,\n",
       " 'knw': 3969,\n",
       " 'tht': 6887,\n",
       " 'keep': 3910,\n",
       " 'payasam': 5134,\n",
       " 'rinu': 5803,\n",
       " 'piggy': 5227,\n",
       " 'awake': 1157,\n",
       " 'bet': 1317,\n",
       " 'sleeping': 6226,\n",
       " 'year': 7685,\n",
       " 'vivek': 7324,\n",
       " 'finished': 2839,\n",
       " 'yet': 7698,\n",
       " 'daddy': 2133,\n",
       " 'pleasure': 5267,\n",
       " 'slap': 6222,\n",
       " 'ass': 1094,\n",
       " 'dick': 2294,\n",
       " 'hey': 3416,\n",
       " 'inconvenient': 3652,\n",
       " 'sis': 6193,\n",
       " 'huh': 3540,\n",
       " 'does': 2385,\n",
       " 'cinema': 1800,\n",
       " 'plus': 5274,\n",
       " 'drink': 2444,\n",
       " 'appeal': 1012,\n",
       " 'tomo': 6966,\n",
       " 'thriller': 6877,\n",
       " 'director': 2332,\n",
       " 'mac': 4303,\n",
       " '30': 427,\n",
       " 'ya': 7670,\n",
       " 'cant': 1621,\n",
       " 'display': 2357,\n",
       " 'internal': 3712,\n",
       " 'subs': 6566,\n",
       " 'extract': 2718,\n",
       " 'his': 3435,\n",
       " 'patty': 5128,\n",
       " 'done': 2405,\n",
       " 'smoke': 6265,\n",
       " 'haul': 3346,\n",
       " 'collect': 1874,\n",
       " 'laptop': 4025,\n",
       " 'gonna': 3171,\n",
       " 'tacos': 6686,\n",
       " 'thk': 6860,\n",
       " 'em': 2557,\n",
       " 'wtc': 7637,\n",
       " 'far': 2762,\n",
       " 'weiyi': 7460,\n",
       " 'goin': 3159,\n",
       " 'dinner': 2325,\n",
       " 'might': 4483,\n",
       " 'unfortunately': 7169,\n",
       " 'found': 2947,\n",
       " 'airport': 880,\n",
       " 'th': 6802,\n",
       " 'yourself': 7723,\n",
       " 'because': 1268,\n",
       " 'miss': 4521,\n",
       " 'envy': 2605,\n",
       " 'everyone': 2655,\n",
       " 'masters': 4388,\n",
       " 'sale': 5895,\n",
       " 'bf': 1328,\n",
       " 'word': 7595,\n",
       " 'checkmate': 1742,\n",
       " 'chess': 1755,\n",
       " 'persian': 5172,\n",
       " 'phrase': 5210,\n",
       " 'shah': 6067,\n",
       " 'maat': 4302,\n",
       " 'which': 7497,\n",
       " 'king': 3946,\n",
       " 'dead': 2175,\n",
       " 'goodmorning': 3178,\n",
       " 'post': 5343,\n",
       " 'l8r': 3990,\n",
       " 'friendship': 2990,\n",
       " 'game': 3055,\n",
       " 'doesn': 2386,\n",
       " 'march': 4368,\n",
       " 'ends': 2575,\n",
       " 'tomorrow': 6968,\n",
       " 'yesterday': 7697,\n",
       " 'office': 4917,\n",
       " 'around': 1061,\n",
       " 'hospital': 3500,\n",
       " 'sunny': 6612,\n",
       " 'california': 1585,\n",
       " 'weather': 7432,\n",
       " 'complete': 1911,\n",
       " 'gist': 3130,\n",
       " 'special': 6372,\n",
       " 'pass': 5111,\n",
       " '09061209465': 188,\n",
       " 'suprman': 6629,\n",
       " 'matrix3': 4400,\n",
       " 'starwars3': 6463,\n",
       " 'etc': 2634,\n",
       " 'bx420': 1569,\n",
       " 'ip4': 3734,\n",
       " '5we': 573,\n",
       " '150pm': 306,\n",
       " 'entry': 2600,\n",
       " '250': 368,\n",
       " 'weekly': 7450,\n",
       " 'comp': 1903,\n",
       " '80086': 638,\n",
       " 'txttowin': 7120,\n",
       " 'kallis': 3892,\n",
       " 'two': 7107,\n",
       " 'odi': 4907,\n",
       " 'amazing': 930,\n",
       " 'quote': 5559,\n",
       " 'sometimes': 6315,\n",
       " 'difficult': 2312,\n",
       " 'decide': 2190,\n",
       " 'whats': 7486,\n",
       " 'wrong': 7632,\n",
       " 'lie': 4108,\n",
       " 'truth': 7069,\n",
       " 'tear': 6740,\n",
       " 'electricity': 2551,\n",
       " 'fml': 2895,\n",
       " 'excellent': 2679,\n",
       " 'ready': 5620,\n",
       " 'moan': 4555,\n",
       " 'ecstasy': 2516,\n",
       " 'battery': 1238,\n",
       " 'low': 4252,\n",
       " 'babe': 1174,\n",
       " 'shower': 6141,\n",
       " 'case': 1654,\n",
       " 'wondering': 7587,\n",
       " 'forgot': 2933,\n",
       " 'something': 6313,\n",
       " 'grandma': 3214,\n",
       " 'parade': 5089,\n",
       " 'people': 5156,\n",
       " 'msgs': 4640,\n",
       " 'addicted': 810,\n",
       " 'msging': 4637,\n",
       " 'bcoz': 1253,\n",
       " 'bslvyl': 1517,\n",
       " 'oops': 4969,\n",
       " 'lazy': 4051,\n",
       " 'type': 7123,\n",
       " 'lect': 4073,\n",
       " 'saw': 5939,\n",
       " 'pouch': 5353,\n",
       " 'honey': 3474,\n",
       " 'predicte': 5387,\n",
       " 'nigeria': 4792,\n",
       " 'many': 4363,\n",
       " 'times': 6907,\n",
       " 'used': 7233,\n",
       " 'monday': 4584,\n",
       " 'nothing': 4851,\n",
       " 'meant': 4425,\n",
       " 'enters': 2593,\n",
       " 'bank': 1206,\n",
       " 'remove': 5711,\n",
       " 'flat': 2867,\n",
       " 'rate': 5595,\n",
       " 'transfered': 7028,\n",
       " 'dollars': 2400,\n",
       " 'removed': 5712,\n",
       " 'banks': 1207,\n",
       " 'differ': 2308,\n",
       " 'charges': 1717,\n",
       " 'trust': 7067,\n",
       " '9ja': 730,\n",
       " 'person': 5174,\n",
       " 'sending': 6025,\n",
       " 'details': 2269,\n",
       " 'yeah': 7684,\n",
       " 'mate': 4393,\n",
       " 'haunt': 3347,\n",
       " 'sorted': 6341,\n",
       " 'im': 3611,\n",
       " 'sound': 6347,\n",
       " 'anyway': 997,\n",
       " 'promoting': 5472,\n",
       " 'hex': 3415,\n",
       " 'joke': 3843,\n",
       " 'compliments': 1918,\n",
       " 'system': 6678,\n",
       " 'side': 6160,\n",
       " 'ill': 3609,\n",
       " 'movie': 4624,\n",
       " 'xy': 7668,\n",
       " 'shop': 6120,\n",
       " 'available': 1142,\n",
       " 'though': 6869,\n",
       " 'sppok': 6423,\n",
       " 'mob': 4556,\n",
       " 'halloween': 3297,\n",
       " 'logo': 4195,\n",
       " 'pic': 5213,\n",
       " 'eerie': 2529,\n",
       " 'tone': 6970,\n",
       " 'card': 1629,\n",
       " 'spook': 6413,\n",
       " 'borrow': 1425,\n",
       " 'bag': 1188,\n",
       " 'thursday': 6890,\n",
       " 'contacted': 1962,\n",
       " 'dating': 2163,\n",
       " 'service': 6043,\n",
       " 'entered': 2592,\n",
       " 'fancy': 2758,\n",
       " '09111032124': 252,\n",
       " 'pobox12n146tf150p': 5284,\n",
       " 'getting': 3111,\n",
       " 'jap': 3797,\n",
       " 'oso': 5010,\n",
       " '12': 280,\n",
       " 'rite': 5808,\n",
       " 'sent': 6033,\n",
       " 'again': 853,\n",
       " 'mystery': 4692,\n",
       " 'solved': 6304,\n",
       " 'opened': 4971,\n",
       " 'email': 2558,\n",
       " 'batch': 1230,\n",
       " 'isn': 3756,\n",
       " 'sweetie': 6659,\n",
       " 'pa': 5057,\n",
       " 'brother': 1506,\n",
       " 'lover': 4244,\n",
       " 'dear1': 2181,\n",
       " 'best1': 1316,\n",
       " 'clos1': 1832,\n",
       " 'lvblefrnd': 4284,\n",
       " 'jstfrnd': 3861,\n",
       " 'cutefrnd': 2118,\n",
       " 'lifpartnr': 4114,\n",
       " 'belovd': 1304,\n",
       " 'swtheart': 6670,\n",
       " 'bstfrnd': 1519,\n",
       " 'rply': 5845,\n",
       " 'enemy': 2577,\n",
       " 'sun0819': 6609,\n",
       " 'posts': 5349,\n",
       " 'hello': 3393,\n",
       " ...}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9fa76074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np=x_train_cv.toarray()\n",
    "X_train_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cb175913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 917,  941, 1570, 2441, 2782, 3132, 3161, 3761, 4586, 4685, 4949,\n",
       "        4959, 5995, 6291, 6387, 6698, 6817, 6859, 6918, 6941, 7151, 7388,\n",
       "        7446, 7527], dtype=int64),)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X_train_np[0]!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "decd3a29",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "7744",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 7744",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train[:\u001b[38;5;241m4\u001b[39m][\u001b[38;5;241m7744\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 7744"
     ]
    }
   ],
   "source": [
    "X_train[:4][7744]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "092ae204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np[0][7411]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb3373",
   "metadata": {},
   "source": [
    "# Train the naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ee1b9849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train_cv,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df271d05",
   "metadata": {},
   "source": [
    "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
    "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1daf022",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "696195fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv=v.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc897b7c",
   "metadata": {},
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "40444e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       963\n",
      "           1       0.97      0.93      0.95       152\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.96      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test_cv)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "267a5088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = [\n",
    "    'Hey mohan, can we get together to watch footbal game tomorrow?',\n",
    "    'Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!'\n",
    "]\n",
    "\n",
    "emails_count = v.transform(emails)\n",
    "\n",
    "model.predict(emails_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa39881",
   "metadata": {},
   "source": [
    "# Train the model using sklearn pipeline and reduce number of lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a202a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer',CountVectorizer()),\n",
    "    ('nb',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a3c4baed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()), (&#x27;nb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()), (&#x27;nb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3870f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       963\n",
      "           1       0.97      0.93      0.95       152\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.96      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffde6cb",
   "metadata": {},
   "source": [
    "# Bag of words: Exercises\n",
    "\n",
    "1.In this Exercise, you are going to classify whether a given movie review is positive or negative.\n",
    "\n",
    "2.you are going to use Bag of words for pre-processing the text and apply different classification algorithms.\n",
    "\n",
    "3.Sklearn CountVectorizer has the inbuilt implementations for Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a3b8d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from  sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d8f65",
   "metadata": {},
   "source": [
    "# About Data: IMDB Dataset\n",
    "\n",
    "Credits: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download\n",
    "\n",
    "This data consists of two columns. - review - sentiment\n",
    "Reviews are the statements given by users after watching the movie.\n",
    "sentiment feature tells whether the given review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "75c4e713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. read the data provided in the same directory with name 'movies_sentiment_data.csv' and store it in df variable\n",
    "\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "#2. print the shape of the data\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "#3. print top 5 datapoints\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dec18850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column \"Category\" which represent 1 if the sentiment is positive or 0 if it is negative\n",
    "df['sentiment'].value_counts()\n",
    "\n",
    "df['category']=df['sentiment'].apply(lambda x: 1 if x=='positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a700f75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25000\n",
       "0    25000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of 'Category' and see whether the Target labels are balanced or not.\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1f66085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the 'train-test' splitting with test size of 20%\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.review, df.category, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3755d15",
   "metadata": {},
   "source": [
    "# Exercise-1\n",
    "\n",
    "1.using sklearn pipeline module create a classification pipeline to classify the movie review's positive or negative.\n",
    "\n",
    "Note:\n",
    "\n",
    "1.use CountVectorizer for pre-processing the text.\n",
    "\n",
    "2.use Random Forest as the classifier with estimators as 50 and criterion as entropy.\n",
    "\n",
    "3.print the classification report.\n",
    "\n",
    "References:\n",
    "\n",
    "1.https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "2.https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ee3e9ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85      4999\n",
      "           1       0.85      0.84      0.84      5001\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),                                                    #initializing the vectorizer\n",
    "    ('random_forest', (RandomForestClassifier(n_estimators=50, criterion='entropy')))      #using the RandomForest classifier\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2637ba6b",
   "metadata": {},
   "source": [
    "As you can see above, for both the classes (positive and negative sentiment) we got more than 80% precision, recall and f1- score. This seems to be an acceptable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d397e5",
   "metadata": {},
   "source": [
    "# Exercise-2\n",
    "\n",
    "1.using sklearn pipeline module create a classification pipeline to classify the movie review's positive or negative..\n",
    "\n",
    "Note:\n",
    "\n",
    "1.use CountVectorizer for pre-processing the text.\n",
    "\n",
    "2.use KNN as the classifier with n_neighbors of 10 and metric as 'euclidean'.\n",
    "\n",
    "3.print the classification report.\n",
    "\n",
    "References:\n",
    "\n",
    "1.https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "2.https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e0de86c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.66      0.66      4999\n",
      "           1       0.66      0.65      0.65      5001\n",
      "\n",
      "    accuracy                           0.66     10000\n",
      "   macro avg       0.66      0.66      0.66     10000\n",
      "weighted avg       0.66      0.66      0.66     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "                \n",
    "     ('vectorizer', CountVectorizer()),   \n",
    "      ('KNN', (KNeighborsClassifier(n_neighbors=10, metric = 'euclidean')))   #using the KNN classifier with 10 neighbors \n",
    "])\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eafc75",
   "metadata": {},
   "source": [
    "Hmmm..here the performance of various metrics (precision, recall etc.) seem to be lower (~60 %). Let's try one more classifier and then discuss why performance is varying so much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6350d1",
   "metadata": {},
   "source": [
    "# Exercise-3\n",
    "\n",
    "1.using sklearn pipeline module create a classification pipeline to classify the movie review's positive or negative..\n",
    "\n",
    "Note:\n",
    "\n",
    "1.use CountVectorizer for pre-processing the text.\n",
    "\n",
    "2.use Multinomial Naive Bayes as the classifier.\n",
    "\n",
    "3.print the classification report.\n",
    "\n",
    "References:\n",
    "\n",
    "1.https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "2.https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "47f47f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85      4999\n",
      "           1       0.87      0.82      0.84      5001\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "                \n",
    "     ('vectorizer', CountVectorizer()),   \n",
    "      ('Multi NB', MultinomialNB())   #using the Multinomial Naive Bayes classifier \n",
    "])\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44b18d",
   "metadata": {},
   "source": [
    "That's great! MultinomialNB model for both the classes (positive and negative sentiment) we got more than 80% precision, recall and f1- score and performed equally good with Random Forest. This seems to be an acceptable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6716ee",
   "metadata": {},
   "source": [
    "# Can you write some observations of why model like KNN fails to produce good results unlike RandomForest and MultinomialNB?\n",
    "\n",
    "As Machine learning algorithms does not work on Text data directly, we need to convert them into numeric vector and feed that into models while training.\n",
    "\n",
    "In this process, we convert text into a very high dimensional numeric vector using the technique of Bag of words.\n",
    "\n",
    "Model like K-Nearest Neighbours(KNN) doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension. In higher dimensional space, the cost to calculate distance becomes expensive and hence impacts the performance of model.\n",
    "\n",
    "The easy calculation of probabilities for the words in corpus(Bag of words) and storing them in contigency table is the major reason for the Multinomial NaiveBayes to be a text classification friendly algorithm.\n",
    "\n",
    "As Random Forest uses Bootstrapping(Row and column Sampling) with many decision tree and overcomes the high variance and overfitting of high dimensional data and also uses feature importance of words for better classifing the categories.\n",
    "\n",
    "Machine Learning is like trial and error scientific method, where we keep trying all the possible algorithms we have and select the one which give good results and satisfy the requirements like latency, interpretability etc.\n",
    "\n",
    "Refer these resources to get good idea:\n",
    "\n",
    "https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\n",
    "https://analyticsindiamag.com/naive-bayes-why-is-it-favoured-for-text-related-tasks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "905ef15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f7049bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "just\n",
      "our\n",
      "the\n",
      "part\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"we just opened our wings, the flying part is coming soon\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "283cb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    no_stop_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    return \" \".join(no_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "db9f36d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'opened wings flying coming soon'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess('we just opened our wings, the flying part is coming soon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eb9c5a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Musk wants time prepare trial'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Musk wants time to prepare for a trial over his\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a29c80b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'divine brother'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"The other is not other but your divine brother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fdf24f",
   "metadata": {},
   "source": [
    "# Remove stop words from pandas dataframe text column\n",
    "Dataset is downloaded from: https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases It contains press releases of different court cases from depart of justice (DOJ). The releases contain information such as outcomes of criminal cases, notable actions taken against felons, or other updates about the current administration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "be1559cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13087, 6)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"doj_press.json\",lines=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5a0b0939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>date</th>\n",
       "      <th>topics</th>\n",
       "      <th>components</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Convicted Bomb Plotter Sentenced to 30 Years</td>\n",
       "      <td>PORTLAND, Oregon. – Mohamed Osman Mohamud, 23,...</td>\n",
       "      <td>2014-10-01T00:00:00-04:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[National Security Division (NSD)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-919</td>\n",
       "      <td>$1 Million in Restitution Payments Announced t...</td>\n",
       "      <td>WASHINGTON – North Carolina’s Waccamaw River...</td>\n",
       "      <td>2012-07-25T00:00:00-04:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1002</td>\n",
       "      <td>$1 Million Settlement Reached for Natural Reso...</td>\n",
       "      <td>BOSTON– A $1-million settlement has been...</td>\n",
       "      <td>2011-08-03T00:00:00-04:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-015</td>\n",
       "      <td>10 Las Vegas Men Indicted \\r\\nfor Falsifying V...</td>\n",
       "      <td>WASHINGTON—A federal grand jury in Las Vegas...</td>\n",
       "      <td>2010-01-08T00:00:00-05:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-898</td>\n",
       "      <td>$100 Million Settlement Will Speed Cleanup Wor...</td>\n",
       "      <td>The U.S. Department of Justice, the U.S. Envir...</td>\n",
       "      <td>2018-07-09T00:00:00-04:00</td>\n",
       "      <td>[Environment]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0     None       Convicted Bomb Plotter Sentenced to 30 Years   \n",
       "1  12-919   $1 Million in Restitution Payments Announced t...   \n",
       "2  11-1002  $1 Million Settlement Reached for Natural Reso...   \n",
       "3   10-015  10 Las Vegas Men Indicted \\r\\nfor Falsifying V...   \n",
       "4   18-898  $100 Million Settlement Will Speed Cleanup Wor...   \n",
       "\n",
       "                                            contents  \\\n",
       "0  PORTLAND, Oregon. – Mohamed Osman Mohamud, 23,...   \n",
       "1    WASHINGTON – North Carolina’s Waccamaw River...   \n",
       "2        BOSTON– A $1-million settlement has been...   \n",
       "3    WASHINGTON—A federal grand jury in Las Vegas...   \n",
       "4  The U.S. Department of Justice, the U.S. Envir...   \n",
       "\n",
       "                        date         topics  \\\n",
       "0  2014-10-01T00:00:00-04:00             []   \n",
       "1  2012-07-25T00:00:00-04:00             []   \n",
       "2  2011-08-03T00:00:00-04:00             []   \n",
       "3  2010-01-08T00:00:00-05:00             []   \n",
       "4  2018-07-09T00:00:00-04:00  [Environment]   \n",
       "\n",
       "                                     components  \n",
       "0            [National Security Division (NSD)]  \n",
       "1  [Environment and Natural Resources Division]  \n",
       "2  [Environment and Natural Resources Division]  \n",
       "3  [Environment and Natural Resources Division]  \n",
       "4  [Environment and Natural Resources Division]  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9ae08f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['topics'].str.len()!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4eabbdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>date</th>\n",
       "      <th>topics</th>\n",
       "      <th>components</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-898</td>\n",
       "      <td>$100 Million Settlement Will Speed Cleanup Wor...</td>\n",
       "      <td>The U.S. Department of Justice, the U.S. Envir...</td>\n",
       "      <td>2018-07-09T00:00:00-04:00</td>\n",
       "      <td>[Environment]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14-1412</td>\n",
       "      <td>14 Indicted in Connection with New England Com...</td>\n",
       "      <td>A 131-count criminal indictment was unsealed t...</td>\n",
       "      <td>2014-12-17T00:00:00-05:00</td>\n",
       "      <td>[Consumer Protection]</td>\n",
       "      <td>[Civil Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17-1419</td>\n",
       "      <td>2017 Southeast Regional Animal Cruelty Prosecu...</td>\n",
       "      <td>The United States Attorney’s Office for the Mi...</td>\n",
       "      <td>2017-12-14T00:00:00-05:00</td>\n",
       "      <td>[Environment]</td>\n",
       "      <td>[Environment and Natural Resources Division, U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15-1562</td>\n",
       "      <td>21st Century Oncology to Pay $19.75 Million to...</td>\n",
       "      <td>21st Century Oncology LLC, has agreed to pay $...</td>\n",
       "      <td>2015-12-18T00:00:00-05:00</td>\n",
       "      <td>[False Claims Act, Health Care Fraud]</td>\n",
       "      <td>[Civil Division]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17-1404</td>\n",
       "      <td>21st Century Oncology to Pay $26 Million to Se...</td>\n",
       "      <td>21st Century Oncology Inc. and certain of its ...</td>\n",
       "      <td>2017-12-12T00:00:00-05:00</td>\n",
       "      <td>[Health Care Fraud, False Claims Act]</td>\n",
       "      <td>[Civil Division, USAO - Florida, Middle]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "4    18-898  $100 Million Settlement Will Speed Cleanup Wor...   \n",
       "7   14-1412  14 Indicted in Connection with New England Com...   \n",
       "19  17-1419  2017 Southeast Regional Animal Cruelty Prosecu...   \n",
       "22  15-1562  21st Century Oncology to Pay $19.75 Million to...   \n",
       "23  17-1404  21st Century Oncology to Pay $26 Million to Se...   \n",
       "\n",
       "                                             contents  \\\n",
       "4   The U.S. Department of Justice, the U.S. Envir...   \n",
       "7   A 131-count criminal indictment was unsealed t...   \n",
       "19  The United States Attorney’s Office for the Mi...   \n",
       "22  21st Century Oncology LLC, has agreed to pay $...   \n",
       "23  21st Century Oncology Inc. and certain of its ...   \n",
       "\n",
       "                         date                                 topics  \\\n",
       "4   2018-07-09T00:00:00-04:00                          [Environment]   \n",
       "7   2014-12-17T00:00:00-05:00                  [Consumer Protection]   \n",
       "19  2017-12-14T00:00:00-05:00                          [Environment]   \n",
       "22  2015-12-18T00:00:00-05:00  [False Claims Act, Health Care Fraud]   \n",
       "23  2017-12-12T00:00:00-05:00  [Health Care Fraud, False Claims Act]   \n",
       "\n",
       "                                           components  \n",
       "4        [Environment and Natural Resources Division]  \n",
       "7                                    [Civil Division]  \n",
       "19  [Environment and Natural Resources Division, U...  \n",
       "22                                   [Civil Division]  \n",
       "23           [Civil Division, USAO - Florida, Middle]  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "74826274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4688, 6)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1e84d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "50fe5c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7b27a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contents_new']=df['contents'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "efea6e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>date</th>\n",
       "      <th>topics</th>\n",
       "      <th>components</th>\n",
       "      <th>contents_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-898</td>\n",
       "      <td>$100 Million Settlement Will Speed Cleanup Wor...</td>\n",
       "      <td>The U.S. Department of Justice, the U.S. Envir...</td>\n",
       "      <td>2018-07-09T00:00:00-04:00</td>\n",
       "      <td>[Environment]</td>\n",
       "      <td>[Environment and Natural Resources Division]</td>\n",
       "      <td>U.S. Department Justice U.S. Environmental Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14-1412</td>\n",
       "      <td>14 Indicted in Connection with New England Com...</td>\n",
       "      <td>A 131-count criminal indictment was unsealed t...</td>\n",
       "      <td>2014-12-17T00:00:00-05:00</td>\n",
       "      <td>[Consumer Protection]</td>\n",
       "      <td>[Civil Division]</td>\n",
       "      <td>131 count criminal indictment unsealed today B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>17-1419</td>\n",
       "      <td>2017 Southeast Regional Animal Cruelty Prosecu...</td>\n",
       "      <td>The United States Attorney’s Office for the Mi...</td>\n",
       "      <td>2017-12-14T00:00:00-05:00</td>\n",
       "      <td>[Environment]</td>\n",
       "      <td>[Environment and Natural Resources Division, U...</td>\n",
       "      <td>United States Attorney Office Middle District ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15-1562</td>\n",
       "      <td>21st Century Oncology to Pay $19.75 Million to...</td>\n",
       "      <td>21st Century Oncology LLC, has agreed to pay $...</td>\n",
       "      <td>2015-12-18T00:00:00-05:00</td>\n",
       "      <td>[False Claims Act, Health Care Fraud]</td>\n",
       "      <td>[Civil Division]</td>\n",
       "      <td>21st Century Oncology LLC agreed pay $ 19.75 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17-1404</td>\n",
       "      <td>21st Century Oncology to Pay $26 Million to Se...</td>\n",
       "      <td>21st Century Oncology Inc. and certain of its ...</td>\n",
       "      <td>2017-12-12T00:00:00-05:00</td>\n",
       "      <td>[Health Care Fraud, False Claims Act]</td>\n",
       "      <td>[Civil Division, USAO - Florida, Middle]</td>\n",
       "      <td>21st Century Oncology Inc. certain subsidiarie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "4    18-898  $100 Million Settlement Will Speed Cleanup Wor...   \n",
       "7   14-1412  14 Indicted in Connection with New England Com...   \n",
       "19  17-1419  2017 Southeast Regional Animal Cruelty Prosecu...   \n",
       "22  15-1562  21st Century Oncology to Pay $19.75 Million to...   \n",
       "23  17-1404  21st Century Oncology to Pay $26 Million to Se...   \n",
       "\n",
       "                                             contents  \\\n",
       "4   The U.S. Department of Justice, the U.S. Envir...   \n",
       "7   A 131-count criminal indictment was unsealed t...   \n",
       "19  The United States Attorney’s Office for the Mi...   \n",
       "22  21st Century Oncology LLC, has agreed to pay $...   \n",
       "23  21st Century Oncology Inc. and certain of its ...   \n",
       "\n",
       "                         date                                 topics  \\\n",
       "4   2018-07-09T00:00:00-04:00                          [Environment]   \n",
       "7   2014-12-17T00:00:00-05:00                  [Consumer Protection]   \n",
       "19  2017-12-14T00:00:00-05:00                          [Environment]   \n",
       "22  2015-12-18T00:00:00-05:00  [False Claims Act, Health Care Fraud]   \n",
       "23  2017-12-12T00:00:00-05:00  [Health Care Fraud, False Claims Act]   \n",
       "\n",
       "                                           components  \\\n",
       "4        [Environment and Natural Resources Division]   \n",
       "7                                    [Civil Division]   \n",
       "19  [Environment and Natural Resources Division, U...   \n",
       "22                                   [Civil Division]   \n",
       "23           [Civil Division, USAO - Florida, Middle]   \n",
       "\n",
       "                                         contents_new  \n",
       "4   U.S. Department Justice U.S. Environmental Pro...  \n",
       "7   131 count criminal indictment unsealed today B...  \n",
       "19  United States Attorney Office Middle District ...  \n",
       "22  21st Century Oncology LLC agreed pay $ 19.75 m...  \n",
       "23  21st Century Oncology Inc. certain subsidiarie...  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6d560c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6286"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.contents[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c3d9acd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4217"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['contents_new'].iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c7b18d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The U.S. Department of Justice, the U.S. Environmental Protection Agency (EPA), and the Rhode Island Department of Environmental Management (RIDEM) announced today that two subsidiaries of Stanley Black & Decker Inc.—Emhart Industries Inc. and Black & Decker Inc.—have agreed to clean up dioxin conta'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.contents[4][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f32412f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U.S. Department Justice U.S. Environmental Protection Agency EPA Rhode Island Department Environmental Management RIDEM announced today subsidiaries Stanley Black Decker Inc.—Emhart Industries Inc. Black Decker Inc.—have agreed clean dioxin contaminated sediment soil Centredale Manor Restoration Pro'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.contents_new[4][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc58e44",
   "metadata": {},
   "source": [
    "# Examples where removing stop words can create a problem\n",
    "\n",
    "(1) Sentiment detection: Not always but in some cases, based on your dataset it can change the sentiment of a sentence if you remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "334136fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good movie'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"this is a good movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e57a75a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good movie'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"this is not a good movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c972736",
   "metadata": {},
   "source": [
    "# (2) Language translation: Say you want to translate following sentence from english to telugu. Before actual translation if you remove stop words and then translate, it will produce horrible result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "872ec4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dhaval'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"how are you doing dhaval?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea182002",
   "metadata": {},
   "source": [
    "# (3) Chat bot or any Q&A system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3b679d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'find yoga mat website help'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"I don't find yoga mat on your website. Can you help?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11bdfc",
   "metadata": {},
   "source": [
    "# Stop Words: Exercise\n",
    "\n",
    "Run this cell to import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "637522ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy and load the model\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ad10a",
   "metadata": {},
   "source": [
    "# Exercise1:\n",
    "\n",
    "From a Given Text, Count the number of stop words in it.\n",
    "\n",
    "Print the percentage of stop word tokens compared to all tokens in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ce1a7a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of stop words in the text is:40\n",
      "percentage of stop words in text:25.0 %\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Thor: Love and Thunder is a 2022 American superhero film based on Marvel Comics featuring the character Thor, produced by Marvel Studios and \n",
    "distributed by Walt Disney Studios Motion Pictures. It is the sequel to Thor: Ragnarok (2017) and the 29th film in the Marvel Cinematic Universe (MCU).\n",
    "The film is directed by Taika Waititi, who co-wrote the script with Jennifer Kaytin Robinson, and stars Chris Hemsworth as Thor alongside Christian Bale, Tessa Thompson,\n",
    "Jaimie Alexander, Waititi, Russell Crowe, and Natalie Portman. In the film, Thor attempts to find inner peace, but must return to action and recruit Valkyrie (Thompson),\n",
    "Korg (Waititi), and Jane Foster (Portman)—who is now the Mighty Thor—to stop Gorr the God Butcher (Bale) from eliminating all gods.\n",
    "'''\n",
    "\n",
    "#step1: Create the object 'doc' for the given text using nlp()\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#step2: define the variables to keep track of stopwords count and total words count\n",
    "stop_words_count=0\n",
    "total_words_count=0\n",
    "\n",
    "\n",
    "#step3: iterate through all the words in the document\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        stop_words_count+=1\n",
    "    total_words_count+=1\n",
    "\n",
    "\n",
    "\n",
    "#step4: print the count of stop words\n",
    "print(f\"total number of stop words in the text is:{stop_words_count}\")\n",
    "\n",
    "    \n",
    "\n",
    "#step5: print the percentage of stop words compared to total words in the text\n",
    "percentage_stop_words=(stop_words_count/total_words_count)*100\n",
    "print(f\"percentage of stop words in text:{percentage_stop_words} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56217b2d",
   "metadata": {},
   "source": [
    "# Exercise2:\n",
    "\n",
    "Spacy default implementation considers \"not\" as a stop word. But in some scenarios removing 'not' will completely change the meaning of the statement/text. For Example, consider these two statements:\n",
    "\n",
    "\n",
    "- this is a good movie       ----> Positive Statement\n",
    "\n",
    "- this is not a good movie   ----> Negative Statement\n",
    "\n",
    "So, after applying stopwords to those 2 texts, both will return \"good movie\" and does not respect the polarity/sentiments of text.\n",
    "\n",
    "Now, your task is to remove this stop word \"not\" in spaCy and help in distinguishing the texts.\n",
    "\n",
    "Hint: GOOGLE IT! Google is your friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dae50e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1:good movie\n",
      "Text2:not good movie\n"
     ]
    }
   ],
   "source": [
    "#use this pre-processing function to pass the text and to remove all the stop words and finally get the cleaned form\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    no_stop_words = [token.text for token in doc if not token.is_stop]\n",
    "    return \" \".join(no_stop_words)       \n",
    "\n",
    "\n",
    "#Step1: remove the stopword 'not' in spacy\n",
    "\n",
    "nlp.vocab['not'].is_stop = False\n",
    "\n",
    "\n",
    "#step2: send the two texts given above into the pre-process function and store the transformed texts\n",
    "\n",
    "positive_text=preprocess('this is a good movie')\n",
    "\n",
    "negative_text=preprocess('this is not a good movie')\n",
    "\n",
    "\n",
    "\n",
    "#step3: finally print those 2 transformed text\n",
    "print(f\"Text1:{positive_text}\")\n",
    "print(f\"Text2:{negative_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe50c8a",
   "metadata": {},
   "source": [
    "# Exercise3:\n",
    "\n",
    "From a given text, output the most frequently used token after removing all the stop word tokens and punctuations in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "20c3dc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum frequency word: India\n"
     ]
    }
   ],
   "source": [
    "text = ''' The India men's national cricket team, also known as Team India or the Men in Blue, represents India in men's international cricket.\n",
    "It is governed by the Board of Control for Cricket in India (BCCI), and is a Full Member of the International Cricket Council (ICC) with Test,\n",
    "One Day International (ODI) and Twenty20 International (T20I) status. Cricket was introduced to India by British sailors in the 18th century, and the \n",
    "first cricket club was established in 1792. India's national cricket team played its first Test match on 25 June 1932 at Lord's, becoming the sixth team to be\n",
    "granted test cricket status.\n",
    "'''\n",
    "\n",
    "\n",
    "#step1: Create the object 'doc' for the given text using nlp()\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "#step2: remove all the stop words and punctuations and store all the remaining tokens in a new list\n",
    "remaining_tokens = []\n",
    "for token in doc:\n",
    "  if token.is_stop or token.is_punct:    #check whether a given token is stop word or punctuations\n",
    "    continue\n",
    "  remaining_tokens.append(token.text)\n",
    "\n",
    "\n",
    "#step3: create a new dictionary and get the frequency of words by iterating through the list which contains stored tokens  \n",
    "frequency_tokens = {}\n",
    "for token in remaining_tokens:\n",
    "  if token != '\\n' and token != ' ':      #As spacy considers new line and empty spaces as seperate token, it's better to ignore them\n",
    "    if token not in frequency_tokens:     #if a particular token occurs for the first time, we initialise it to 1\n",
    "      frequency_tokens[token] = 1\n",
    "    else:\n",
    "      frequency_tokens[token] += 1        #if a partcular token is already present, then increment by 1 based on value already presented\n",
    "\n",
    "\n",
    "#step4: get the maximum frequency word\n",
    "max_freq_word = max(frequency_tokens.keys(), key=(lambda key: frequency_tokens[key]))\n",
    "\n",
    "\n",
    "#step5: finally print the result\n",
    "print(f\"Maximum frequency word: {max_freq_word}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc872e",
   "metadata": {},
   "source": [
    "# Codebasics - Bag of N grams tutorial\n",
    "\n",
    "Let's first understand how to generate n-grams using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e359e94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 5, 'hathodawala': 1, 'is': 2, 'looking': 4, 'for': 0, 'job': 3}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v=CountVectorizer()\n",
    "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6089fba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 9,\n",
       " 'hathodawala': 2,\n",
       " 'is': 4,\n",
       " 'looking': 7,\n",
       " 'for': 0,\n",
       " 'job': 6,\n",
       " 'thor hathodawala': 10,\n",
       " 'hathodawala is': 3,\n",
       " 'is looking': 5,\n",
       " 'looking for': 8,\n",
       " 'for job': 1}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(ngram_range=(1,2))\n",
    "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7e4fc1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 12,\n",
       " 'hathodawala': 2,\n",
       " 'is': 5,\n",
       " 'looking': 9,\n",
       " 'for': 0,\n",
       " 'job': 8,\n",
       " 'thor hathodawala': 13,\n",
       " 'hathodawala is': 3,\n",
       " 'is looking': 6,\n",
       " 'looking for': 10,\n",
       " 'for job': 1,\n",
       " 'thor hathodawala is': 14,\n",
       " 'hathodawala is looking': 4,\n",
       " 'is looking for': 7,\n",
       " 'looking for job': 11}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(ngram_range=(1,3))\n",
    "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73cdf4",
   "metadata": {},
   "source": [
    "# We will not take a simple collection of text documents, preprocess them to remove stop words, lemmatize etc and then generate bag of 1 grams and 2 grams from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8336a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Thor ate pizza\",\n",
    "    \"Loki is tall\",\n",
    "    \"Loki is eating pizza\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "139864a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    doc=nlp(text)\n",
    "    \n",
    "    filtered_tokens=[]\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_stop or not token.is_punct:\n",
    "            filtered_tokens.append(token.lemma_)\n",
    "            \n",
    "    return \" \".join(filtered_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "53b66b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thor eat pizza'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Thor ate pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c0b4a553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loki be eat pizza'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Loki is eating pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ea033b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thor eat pizza', 'Loki be tall', 'Loki be eat pizza']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_processed = [preprocess(text) for text in corpus]\n",
    "corpus_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3d487fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 9,\n",
       " 'eat': 3,\n",
       " 'pizza': 7,\n",
       " 'thor eat': 10,\n",
       " 'eat pizza': 4,\n",
       " 'loki': 5,\n",
       " 'be': 0,\n",
       " 'tall': 8,\n",
       " 'loki be': 6,\n",
       " 'be tall': 2,\n",
       " 'be eat': 1}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=CountVectorizer(ngram_range=(1,2))\n",
    "v.fit(corpus_processed)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e066ac",
   "metadata": {},
   "source": [
    "Now generate bag of n gram vector for few sample documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b47fc337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Thor eat pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5c3c6f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Hulk Eat Pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ff12b",
   "metadata": {},
   "source": [
    "# News Category Classification Problem\n",
    "\n",
    "Okay now that we know basics of BAG of n grams vectorizer 😎 It is the time to work on a real problem. Here we want to do a news category classification. We will use bag of n-grams and traing a machine learning model that can categorize any news into one of the following categories,\n",
    "\n",
    "BUSINESS\n",
    "SPORTS\n",
    "CRIME\n",
    "SCIENCE\n",
    "Dataset\n",
    "Dataset Credits: https://www.kaggle.com/code/hengzheng/news-category-classifier-val-acc-0-65\n",
    "\n",
    "This data consists of two columns. - Text - Category\n",
    "\n",
    "Text is a news article\n",
    "\n",
    "Category can be one of these 4: 'BUSINESS', 'SPORTS', 'CRIME', 'SCIENCE', to keep things simple I trimmed additional categories from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "69a828d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12695, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Watching Schrödinger's Cat Die University of C...</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WATCH: Freaky Vortex Opens Up In Flooded Lake</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entrepreneurs Today Don't Need a Big Budget to...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These Roads Could Recharge Your Electric Car A...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Civilian 'Guard' Fires Gun While 'Protecting' ...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0  Watching Schrödinger's Cat Die University of C...   SCIENCE\n",
       "1     WATCH: Freaky Vortex Opens Up In Flooded Lake    SCIENCE\n",
       "2  Entrepreneurs Today Don't Need a Big Budget to...  BUSINESS\n",
       "3  These Roads Could Recharge Your Electric Car A...  BUSINESS\n",
       "4  Civilian 'Guard' Fires Gun While 'Protecting' ...     CRIME"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_json('news_dataset.json')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "10765412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BUSINESS    4254\n",
       "SPORTS      4167\n",
       "CRIME       2893\n",
       "SCIENCE     1381\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f19b7",
   "metadata": {},
   "source": [
    "# Handle class imbalance\n",
    "As you can see above, SCIENCE category has almost 1/3rd data samples compared to BUSINESS and SPORTS categories. I initially trained a model without handling the imbalanced I saw a lower f1-score for SCIENCE category. Hence we need to address this imbalanced.\n",
    "\n",
    "There are various ways of handling class imbalance which I have discussed in this video: https://www.youtube.com/watch?v=JnlM4yLFNuo\n",
    "\n",
    "Out of those techniques, I will use undersampling technique here.\n",
    "\n",
    "In undersampling, we take a minor class and sample those many samples from other classes, this means we are not utilizing all the data samples for training and in ML world - Not using all the data for training is considered a SIN! 😵 In real life, you are advised to use a technique such as SMOTE so that you can utilize all of your dataset for the training but since this tutorial is more about bag of n-grams then class imbalance itself, I'd go with a simple technique of undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b4ed5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 1381\n",
    "\n",
    "df_business=df[df.category == 'BUSINESS'].sample(min_samples, random_state=2022)\n",
    "df_sports=df[df.category == 'SPORTS'].sample(min_samples, random_state=2022)\n",
    "df_crime=df[df.category == 'CRIME'].sample(min_samples, random_state=2022)\n",
    "df_science=df[df.category == 'SCIENCE'].sample(min_samples, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "071a54be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BUSINESS    1381\n",
       "SPORTS      1381\n",
       "CRIME       1381\n",
       "SCIENCE     1381\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced = pd.concat([df_business,df_sports,df_crime,df_science],axis=0)\n",
    "df_balanced.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605987e",
   "metadata": {},
   "source": [
    "# Convert text category to a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "07260c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11967</th>\n",
       "      <td>GCC Business Leaders Remain Confident in the F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>From the Other Side; an Honest Review from Emp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>Mike McDerment, CEO of FreshBooks, Talks About...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>How to Market Your Business While Traveling th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>How to Leverage Intuition in Decision-making I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>Aquarium To Monitor Animals' Behavior Changes ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>How Google Glass Could Save Lives In The Hospi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>Honda's Gravity Modification Research For us A...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11428</th>\n",
       "      <td>EVERYONE Loves Alternative Facts THE POWER OF ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8101</th>\n",
       "      <td>From Cooking to Conservation: Women Take Actio...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5524 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category\n",
       "11967  GCC Business Leaders Remain Confident in the F...         0\n",
       "2912   From the Other Side; an Honest Review from Emp...         0\n",
       "3408   Mike McDerment, CEO of FreshBooks, Talks About...         0\n",
       "502    How to Market Your Business While Traveling th...         0\n",
       "5279   How to Leverage Intuition in Decision-making I...         0\n",
       "...                                                  ...       ...\n",
       "2178   Aquarium To Monitor Animals' Behavior Changes ...         3\n",
       "5682   How Google Glass Could Save Lives In The Hospi...         3\n",
       "1643   Honda's Gravity Modification Research For us A...         3\n",
       "11428  EVERYONE Loves Alternative Facts THE POWER OF ...         3\n",
       "8101   From Cooking to Conservation: Women Take Actio...         3\n",
       "\n",
       "[5524 rows x 2 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced = df_balanced.replace({'category':{'BUSINESS':0,'SPORTS':1,'CRIME':2,'SCIENCE':3}})\n",
    "df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6e9c5",
   "metadata": {},
   "source": [
    "# Build a model with original text (no pre processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5dd422fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(df_balanced.text,df_balanced.category,test_size=0.2,random_state=2022,stratify=df_balanced.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3e59234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4419,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7589     Ovulating Women Prefer Images of Penetration O...\n",
       "10442    Scientists Discover Spooky Influence On Baby N...\n",
       "8792     Olympic Race Walker Steps Up To Propose To His...\n",
       "1733     Beloved Bipedal Bear Named Pedals Believed Kil...\n",
       "2526     Elizabeth Smart Gave Birth To Baby Girl, Fathe...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1e3a716f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1105\n",
       "2    1105\n",
       "0    1105\n",
       "1    1104\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "abea6877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    277\n",
       "0    276\n",
       "3    276\n",
       "2    276\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742282e",
   "metadata": {},
   "source": [
    "# Attempt 1 : Use 1-gram which is nothing but a Bag Of Words (BOW) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "80c2e71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.75      0.81       321\n",
      "           1       0.80      0.93      0.86       240\n",
      "           2       0.90      0.83      0.86       300\n",
      "           3       0.80      0.90      0.85       244\n",
      "\n",
      "    accuracy                           0.84      1105\n",
      "   macro avg       0.84      0.85      0.84      1105\n",
      "weighted avg       0.85      0.84      0.84      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bow',CountVectorizer(ngram_range=(1,1))),\n",
    "    ('Multi NB',MultinomialNB())\n",
    "])\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9789bf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3716     African Nation Slaps Exxon With Fine Nearly 7 ...\n",
       "608      These Cringe-Worthy Stories Show It Can Be Har...\n",
       "11172    LISTEN: The Accidental Discovery That Proved T...\n",
       "1346     Build Loyalty -- The Cost -- $00.00 Remember y...\n",
       "1356     Man Killed By Michigan Police Wasn't Targeting...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481a927",
   "metadata": {},
   "source": [
    "BUSINESS: 0\n",
    "SPORTS: 1\n",
    "CRIME: 2\n",
    "SCIENCE: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fa9f78f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3716     0\n",
       "608      3\n",
       "11172    3\n",
       "1346     0\n",
       "1356     2\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ff4298f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 3, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8a9a5",
   "metadata": {},
   "source": [
    "# Attempt 2 : Use 1-gram and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "35a6dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.69      0.78       361\n",
      "           1       0.74      0.95      0.83       217\n",
      "           2       0.88      0.82      0.85       295\n",
      "           3       0.78      0.92      0.84       232\n",
      "\n",
      "    accuracy                           0.82      1105\n",
      "   macro avg       0.82      0.85      0.83      1105\n",
      "weighted avg       0.84      0.82      0.82      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vectorizer_1_2_bow',CountVectorizer(ngram_range=(1,2))),\n",
    "    ('Multi NB',MultinomialNB())\n",
    "])\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fe640",
   "metadata": {},
   "source": [
    "# Attempt 3 : Use 1-gram to trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3d74382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.67      0.77       378\n",
      "           1       0.73      0.96      0.83       212\n",
      "           2       0.87      0.83      0.85       288\n",
      "           3       0.76      0.93      0.83       227\n",
      "\n",
      "    accuracy                           0.82      1105\n",
      "   macro avg       0.82      0.84      0.82      1105\n",
      "weighted avg       0.83      0.82      0.82      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vectorizer_1_3_grams',CountVectorizer(ngram_range=(1,3))),\n",
    "    ('Multi_NB',MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327ead8",
   "metadata": {},
   "source": [
    "# Use text pre-processing to remove stop words, punctuations and apply lemmatization\n",
    "\n",
    "You may wonder, we have not done any text-processing yet to remove stop words, punctuations, apply lemmatization etc. Well we wanted to train the model without any preprocessing first and check the performance. Now we will re-do same thing but with preprocessing of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0cd21c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced['preprocessed_txt']=df_balanced['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "eef5f874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>preprocessed_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11967</th>\n",
       "      <td>GCC Business Leaders Remain Confident in the F...</td>\n",
       "      <td>0</td>\n",
       "      <td>GCC Business leader remain confident in the Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>From the Other Side; an Honest Review from Emp...</td>\n",
       "      <td>0</td>\n",
       "      <td>from the other Side ; an Honest Review from Em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>Mike McDerment, CEO of FreshBooks, Talks About...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mike McDerment , CEO of FreshBooks , Talks abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>How to Market Your Business While Traveling th...</td>\n",
       "      <td>0</td>\n",
       "      <td>how to market your business while travel the W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>How to Leverage Intuition in Decision-making I...</td>\n",
       "      <td>0</td>\n",
       "      <td>how to leverage intuition in decision - making...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category  \\\n",
       "11967  GCC Business Leaders Remain Confident in the F...         0   \n",
       "2912   From the Other Side; an Honest Review from Emp...         0   \n",
       "3408   Mike McDerment, CEO of FreshBooks, Talks About...         0   \n",
       "502    How to Market Your Business While Traveling th...         0   \n",
       "5279   How to Leverage Intuition in Decision-making I...         0   \n",
       "\n",
       "                                        preprocessed_txt  \n",
       "11967  GCC Business leader remain confident in the Fa...  \n",
       "2912   from the other Side ; an Honest Review from Em...  \n",
       "3408   Mike McDerment , CEO of FreshBooks , Talks abo...  \n",
       "502    how to market your business while travel the W...  \n",
       "5279   how to leverage intuition in decision - making...  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c3e311b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced.preprocessed_txt, \n",
    "    df_balanced.category, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df_balanced.category\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "abba0245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4419,)\n",
      "7589     ovulate Women prefer Images of Penetration ove...\n",
      "10442    scientist discover Spooky Influence on Baby Na...\n",
      "8792     Olympic Race Walker step up to Propose to his ...\n",
      "1733     Beloved Bipedal Bear name Pedals believe kill ...\n",
      "2526     Elizabeth Smart give Birth to Baby Girl , Fath...\n",
      "Name: preprocessed_txt, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "49f8f5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1105\n",
       "2    1105\n",
       "0    1105\n",
       "1    1104\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "666ee19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    277\n",
       "0    276\n",
       "3    276\n",
       "2    276\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f178b884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.92      0.79       276\n",
      "           1       0.95      0.76      0.84       277\n",
      "           2       0.84      0.86      0.85       276\n",
      "           3       0.92      0.77      0.84       276\n",
      "\n",
      "    accuracy                           0.83      1105\n",
      "   macro avg       0.85      0.83      0.83      1105\n",
      "weighted avg       0.85      0.83      0.83      1105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bow', CountVectorizer(ngram_range = (1, 2))),        #using the ngram_range parameter \n",
    "    ('Multi NB', MultinomialNB())\n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288e305",
   "metadata": {},
   "source": [
    "If you compare above classification report for (1,2) gram with the one from unprocessed text, you will find some improvement in the model that uses preprocessed cleaned up text. Hence we can conclude that for this particular problem using preprocessing (removing stop words, lemmatization) is improving the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb64e0e",
   "metadata": {},
   "source": [
    "# Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "66ec6d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255,   4,  10,   7],\n",
       "       [ 31, 210,  30,   6],\n",
       "       [ 30,   4, 237,   5],\n",
       "       [ 55,   3,   5, 213]], dtype=int64)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cd3306d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(95.72222222222221, 0.5, 'Truth')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm,annot=True,fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf87c9",
   "metadata": {},
   "source": [
    "# Bag of n_grams: Exercise\n",
    "\n",
    "Fake news refers to misinformation or disinformation in the country which is spread through word of mouth and more recently through digital communication such as What's app messages, social media posts, etc.\n",
    "\n",
    "Fake news spreads faster than Real news and creates problems and fear among groups and in society.\n",
    "\n",
    "We are going to address these problems using classical NLP techniques and going to classify whether a given message/ text is Real or Fake Message.\n",
    "\n",
    "You will use a Bag of n-grams to pre-process the text and apply different classification algorithms.\n",
    "\n",
    "Sklearn CountVectorizer has the inbuilt implementations for Bag of Words.\n",
    "\n",
    "\n",
    "#About Data: Fake News Detection\n",
    "\n",
    "Credits: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "This data consists of two columns. - Text - label\n",
    "\n",
    "Text is the statements or messages regarding a particular event/situation.\n",
    "\n",
    "label feature tells whether the given Text is Fake or Real.\n",
    "\n",
    "As there are only 2 classes, this problem comes under the Binary Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c448b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9900, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake\n",
       "1  U.S. conservative leader optimistic of common ...  Real\n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real\n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake\n",
       "4  Democrats say Trump agrees to work on immigrat...  Real"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "#read the dataset with name \"Fake_Real_Data.csv\" and store it in a variable df\n",
    "df=pd.read_csv('Fake_Real_Data.csv')\n",
    "\n",
    "\n",
    "\n",
    "#print the shape of dataframe\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "#print top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c643fe15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fake    5000\n",
       "Real    4900\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of labels \n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4f1aa066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label  Label_num\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake          0\n",
       "1  U.S. conservative leader optimistic of common ...  Real          1\n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real          1\n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake          0\n",
       "4  Democrats say Trump agrees to work on immigrat...  Real          1"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add the new column \"label_num\" which gives a unique number to each of these labels \n",
    "df['Label_num']=df['label'].map({'Fake':0,'Real':1})\n",
    "\n",
    "#check the results with top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ed326",
   "metadata": {},
   "source": [
    "# Modelling without Pre-processing Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9ad17e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import train-test-split from sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too\n",
    "X_train,X_test,y_train,y_test = train_test_split(df.Text,df.Label_num,test_size=0.2,random_state=2022,stratify=df.Label_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0253a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7920,)\n",
      "(1980,)\n"
     ]
    }
   ],
   "source": [
    "#print the shapes of X_train and X_test\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff8c26",
   "metadata": {},
   "source": [
    "# Attempt 1 :\n",
    "\n",
    "using sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "## Note:\n",
    "\n",
    "using CountVectorizer with unigram, bigram, and trigrams.\n",
    "use KNN as the classifier with n_neighbors of 10 and metric as 'euclidean' distance.\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "706e846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.96      0.65       507\n",
      "           1       0.98      0.65      0.78      1473\n",
      "\n",
      "    accuracy                           0.73      1980\n",
      "   macro avg       0.74      0.81      0.72      1980\n",
      "weighted avg       0.86      0.73      0.75      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bow',CountVectorizer(ngram_range=(1,3))),\n",
    "    ('KNN',KNeighborsClassifier(n_neighbors=10,metric='euclidean'))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf80fc",
   "metadata": {},
   "source": [
    "# Attempt 2 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "## Note:\n",
    "\n",
    "using CountVectorizer with unigram, bigram, and trigrams.\n",
    "use KNN as the classifier with n_neighbors of 10 and metric as 'cosine' distance.\n",
    "print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8f32ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.99      0.71       557\n",
      "           1       1.00      0.69      0.81      1423\n",
      "\n",
      "    accuracy                           0.77      1980\n",
      "   macro avg       0.77      0.84      0.76      1980\n",
      "weighted avg       0.87      0.77      0.78      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bow',CountVectorizer(ngram_range=(1,3))),\n",
    "    ('KNN',KNeighborsClassifier(n_neighbors=10,metric='cosine'))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b24171",
   "metadata": {},
   "source": [
    "# Attempt 3 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "## Note:\n",
    "\n",
    "using CountVectorizer with only trigrams.\n",
    "use RandomForest as the classifier.\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3168010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       994\n",
      "           1       0.99      0.99      0.99       986\n",
      "\n",
      "    accuracy                           0.99      1980\n",
      "   macro avg       0.99      0.99      0.99      1980\n",
      "weighted avg       0.99      0.99      0.99      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_bow',CountVectorizer(ngram_range=(3,3))),\n",
    "    ('RandomForest',RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7b7a7",
   "metadata": {},
   "source": [
    "# Attempt 4 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "## Note:\n",
    "\n",
    "using CountVectorizer with both unigram and bigrams.\n",
    "use Multinomial Naive Bayes as the classifier with an alpha value of 0.75.\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ff557db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1000\n",
      "           1       0.99      0.98      0.99       980\n",
      "\n",
      "    accuracy                           0.99      1980\n",
      "   macro avg       0.99      0.99      0.99      1980\n",
      "weighted avg       0.99      0.99      0.99      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_trigrams', CountVectorizer(ngram_range = (1, 2))),        #using the ngram_range parameter \n",
    "     ('Multi NB', MultinomialNB(alpha = 0.75))         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6578632",
   "metadata": {},
   "source": [
    "# Use text pre-processing to remove stop words, punctuations and apply lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a4a5904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    filtered_tokens=[]\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column \"preprocessed_txt\" and use the utility function above to get the clean data\n",
    "# this will take some time, please be patient\n",
    "df['preprocessed_txt'] = df['Text'].apply(preprocess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ce6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906c15c",
   "metadata": {},
   "source": [
    "# Build a model with pre processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e19256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too\n",
    "#Note: Make sure to use only the \"preprocessed_txt\" column for splitting\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.preprocessed_txt, \n",
    "    df.Label_num,\n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022,\n",
    "    stratify=df.Label_num\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03f5e8",
   "metadata": {},
   "source": [
    "# Let's check the scores with our best model till now\n",
    "\n",
    "Random Forest\n",
    "\n",
    "## Attempt1 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "### Note:\n",
    "\n",
    "using CountVectorizer with only trigrams.\n",
    "\n",
    "use RandomForest as the classifier.\n",
    "\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('count_vectorizer',CountVectorizer(ngram_range=(3,3))),\n",
    "    ('random_forest',RandomForestClassifier())\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950e58e",
   "metadata": {},
   "source": [
    "# Attempt2 :\n",
    "\n",
    "using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "## Note:\n",
    "\n",
    "using CountVectorizer with unigram, bigram, and trigrams.\n",
    "\n",
    "use RandomForest as the classifier.\n",
    "\n",
    "print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a513c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_n_grams', CountVectorizer(ngram_range = (1, 3))),                       #using the ngram_range parameter \n",
    "    ('random_forest', (RandomForestClassifier()))         \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally print the confusion matrix for the best model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8851f",
   "metadata": {},
   "source": [
    "# Final Observations\n",
    "\n",
    "As machine learning algorithms do not work on text data directly, we need to convert them into numeric vectors and feed that into models while training.\n",
    "\n",
    "In this process, we convert text into a very high dimensional numeric vector using the technique of Bag of words and we use sklearn CountVectorizer for this.\n",
    "\n",
    "## Without Pre-Processing Data\n",
    "\n",
    "From the above in most of the cases, we can see that when we have the count vectorizer above trigrams or at trigrams, the performance keeps degrading. The major possible reason for this as the ngram_range keeps increasing, the number of dimensions/features (possible combination of words) also increases enormously and models have the risk of overfitting and resulting in terrible performance.\n",
    "\n",
    "For this reason, models like KNN failed terribly when performed with trigrams and using the euclidean distance. K-Nearest Neighbours(KNN) doesn't work well with high-dimensional data because, with a large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension. In higher dimensional space, the cost to calculate distance becomes expensive and hence impacts the performance of the model. It performed well for class 1 and had terrible results for Class 0.\n",
    "\n",
    "Both recall and F1 scores increase better when trained with the same KNN model but with cosine distance as cosine distance does not get influenced by the number of dimensions as it uses the angle better the two text vectors to calculate the similarity.\n",
    "\n",
    "With respect to Naive and RandomForest models, both performed really well, and random forest with trigrams has a better edge on the recall metric.\n",
    "\n",
    "As Random Forest uses Bootstrapping(row and column Sampling) with many decision trees and overcomes the high variance and overfitting of high dimensional data and also uses feature importance of words for better classifying the categories.\n",
    "\n",
    "The easy calculation of probabilities for the words in the corpus(Bag of words) and storing them in a contingency table is the major reason for the Multinomial NaiveBayes to be a text classification friendly algorithm.\n",
    "\n",
    "## With Pre-Processing Data\n",
    "\n",
    "Have trained the best model RandomForest on the pre-processed data, but RandomForest with trigrams fails to produce the same results here.\n",
    "\n",
    "But the same randomForest with Unigram to Trigram features helps to produce very amazing results and is tops in the entire list with very good F1 scores and Recall scores.\n",
    "\n",
    "Machine Learning is like a trial and error scientific method, where we keep trying all the possible algorithms we have and select the one which gives good results and satisfies the requirements like latency, interpretability, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2088a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c20f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf22d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3c03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b4fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
